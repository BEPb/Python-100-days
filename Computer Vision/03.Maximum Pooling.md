### Maximum Pooling
Введение
В Уроке 2 мы начали обсуждение того, как база в свертке выполняет извлечение признаков. Мы узнали о том, как первые 
две операции в этом процессе происходят на уровне Conv2D с активацией relu. 

В этом уроке мы рассмотрим третью (и последнюю) операцию в этой последовательности: уплотнение с максимальным 
объединением, которое в Keras выполняется слоем MaxPool2D. 

### Конденсируйте с максимальным объединением
Добавление шага уплотнения к модели, которая у нас была раньше, даст нам следующее:
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Conv2D(filters=64, kernel_size=3), # activation is None
    layers.MaxPool2D(pool_size=2),
    # More layers follow
])
```

Слой MaxPool2D очень похож на слой Conv2D, за исключением того, что он использует простую функцию максимума вместо 
ядра с параметром pool_size, аналогичным kernel_size. Однако у слоя MaxPool2D нет обучаемых весов, как у сверточного 
слоя в ядре.  

Давайте еще раз посмотрим на фигуру извлечения из прошлого урока. Помните, что MaxPool2D - это этап уплотнения.

Обратите внимание, что после применения функции ReLU (обнаружение) карта функций заканчивается большим «мертвым 
пространством», то есть большими областями, содержащими только нули (черные области на изображении). Необходимость 
переносить эти 0 активаций по всей сети увеличила бы размер модели без добавления много полезной информации. Вместо 
этого мы хотели бы сжать карту функций, чтобы сохранить только самую полезную часть - саму функцию.   

Фактически, это то, что делает максимальный пул. Максимальный пул берет патч активаций в исходной карте функций и 
заменяет их максимальной активацией в этом патче. 

Максимальный пул заменяет патч максимальным значением в этом патче.
При применении после активации ReLU имеет эффект «усиливающих» свойств. На этапе объединения доля активных пикселей увеличивается до нуля.

Пример - применение максимального объединения
Давайте добавим этап «уплотнения» к извлечению признаков, которое мы сделали в примере из Урока 2. Эта следующая 
скрытая ячейка вернет нас туда, где мы остановились. 
```python
import tensorflow as tf
import matplotlib.pyplot as plt
import warnings

plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells

# Read image
image_path = '../input/computer-vision-resources/car_feature.jpg'
image = tf.io.read_file(image_path)
image = tf.io.decode_jpeg(image)

# Define kernel
kernel = tf.constant([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1],
], dtype=tf.float32)

# Reformat for batch compatibility.
image = tf.image.convert_image_dtype(image, dtype=tf.float32)
image = tf.expand_dims(image, axis=0)
kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])

# Filter step
image_filter = tf.nn.conv2d(
    input=image,
    filters=kernel,
    # we'll talk about these two in the next lesson!
    strides=1,
    padding='SAME'
)

# Detect step
image_detect = tf.nn.relu(image_filter)

# Show what we have so far
plt.figure(figsize=(12, 6))
plt.subplot(131)
plt.imshow(tf.squeeze(image), cmap='gray')
plt.axis('off')
plt.title('Input')
plt.subplot(132)
plt.imshow(tf.squeeze(image_filter))
plt.axis('off')
plt.title('Filter')
plt.subplot(133)
plt.imshow(tf.squeeze(image_detect))
plt.axis('off')
plt.title('Detect')
plt.show();
```

Мы будем использовать еще одну из функций в tf.nn для применения шага объединения, tf.nn.pool. Это функция Python, 
которая делает то же самое, что и слой MaxPool2D, который вы используете при построении модели, но, будучи простой 
функцией, ее проще использовать напрямую.   

```python
import tensorflow as tf

image_condense = tf.nn.pool(
    input=image_detect, # image in the Detect step above
    window_shape=(2, 2),
    pooling_type='MAX',
    # we'll see what these do in the next lesson!
    strides=(2, 2),
    padding='SAME',
)

plt.figure(figsize=(6, 6))
plt.imshow(tf.squeeze(image_condense))
plt.axis('off')
plt.show();
```
Довольно круто! Надеюсь, вы увидите, как этап объединения смог усилить эту функцию за счет уплотнения изображения 
вокруг наиболее активных пикселей. 

### Инвариантность перевода
Мы назвали нулевые пиксели «неважными». Означает ли это, что они вообще не несут никакой информации? Фактически, 
нулевые пиксели несут позиционную информацию. Пустое пространство по-прежнему позиционирует элемент на изображении. 
Когда MaxPool2D удаляет некоторые из этих пикселей, он удаляет некоторую позиционную информацию из карты функций. 
Это дает свёртке свойство, называемое инвариантностью трансляции. Это означает, что свёртка с максимальным 
объединением не будет различать элементы по их расположению на изображении. («Перевод» - это математическое слово, 
обозначающее изменение положения чего-либо без его поворота или изменения его формы или размера.)     
 
Посмотрите, что происходит, когда мы неоднократно применяем максимальный пул к следующей карте функций.


Две точки на исходном изображении стали неразличимы после многократного объединения. Другими словами, объединение 
уничтожило часть их позиционной информации. Поскольку сеть больше не может различать их на картах признаков, она не 
может различать их и на исходном изображении: она стала инвариантной к этой разнице в положении.  

Фактически, объединение только создает инвариантность трансляции в сети на небольших расстояниях, как в случае с 
двумя точками на изображении. Объекты, которые начинаются далеко друг от друга, останутся отличными после 
объединения; была потеряна только часть позиционной информации, но не вся.   

Эта инвариантность к небольшим различиям в положении элементов - хорошее свойство классификатора изображений. Просто 
из-за различий в перспективе или кадре объекты одного и того же типа могут располагаться в разных частях исходного 
изображения, но мы все равно хотели бы, чтобы классификатор распознал, что они одинаковы. Поскольку эта 
инвариантность встроена в сеть, мы можем обойтись без использования гораздо меньшего количества данных для обучения: 
нам больше не нужно учить ее игнорировать эту разницу. Это дает сверточным сетям большое преимущество в 
эффективности по сравнению с сетями с плотными слоями. (Вы увидите еще один способ получить инвариантность бесплатно 
в Уроке 6 с помощью увеличения данных!)      

### Вывод
В этом уроке мы узнали о последнем этапе извлечения признаков: уплотнение с помощью MaxPool2D. В Уроке 4 мы закончим 
обсуждение свертки и объединения с помощью скользящих окон. 

