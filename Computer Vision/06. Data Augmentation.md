# Data Augmentation
Введение
Теперь, когда вы изучили основы сверточных классификаторов, вы готовы перейти к более сложным темам.

В этом уроке вы научитесь трюку, который может повысить эффективность ваших классификаторов изображений: он 
называется увеличением данных. 

### Полезность фальшивых данных
Лучший способ повысить производительность модели машинного обучения - обучить ее большему количеству данных. Чем 
больше примеров модель должна извлечь, тем лучше она сможет распознать, какие различия в изображениях имеют значение,
а какие нет. Больше данных помогает модели лучше обобщать.  

Один из простых способов получить больше данных - использовать данные, которые у вас уже есть. Если мы сможем 
преобразовать изображения в нашем наборе данных таким образом, чтобы сохранить класс, мы сможем научить наш 
классификатор игнорировать такие преобразования. Например, то, смотрит ли автомобиль влево или вправо на фотографии, 
не меняет того факта, что это Автомобиль, а не Грузовик. Итак, если мы дополним наши обучающие данные перевернутыми 
изображениями, наш классификатор узнает, что «влево или вправо» - это разница, которую он должен игнорировать.    
 
И в этом вся идея увеличения данных: добавьте дополнительные поддельные данные, которые достаточно похожи на 
настоящие, и ваш классификатор улучшится. 

### Использование увеличения данных
Обычно при пополнении набора данных используется много видов преобразований. Они могут включать поворот изображения, 
настройку цвета или контрастности, деформацию изображения или многое другое, обычно применяемое в сочетании. Вот 
пример различных способов преобразования одного изображения.   

Шестнадцать преобразований единого образа автомобиля.
Увеличение данных обычно выполняется онлайн, то есть по мере того, как изображения загружаются в сеть для обучения. 
Напомним, что обучение обычно проводится на мини-пакетах данных. Так может выглядеть пакет из 16 изображений при 
использовании увеличения данных.  

Пакет из 16 изображений с применением различных случайных преобразований.
Каждый раз, когда изображение используется во время обучения, применяется новое случайное преобразование. Таким 
образом, модель всегда видит что-то немного иное, чем то, что она видела раньше. Эта дополнительная дисперсия в 
обучающих данных помогает модели на новых данных.  

Однако важно помнить, что не каждое преобразование будет полезно для данной проблемы. Самое главное, какие бы 
преобразования вы ни использовали, не следует смешивать классы. Например, если вы тренируете распознаватель цифр, 
при вращении изображений будут смешиваться цифры 9 и 6. В конце концов, лучший подход к поиску хороших дополнений 
такой же, как и к большинству проблем машинного обучения: попробуйте и убедитесь!   

Пример - обучение с расширением данных
Keras позволяет вам пополнять ваши данные двумя способами. Первый способ - включить его в конвейер данных с помощью 
такой функции, как ImageDataGenerator. Второй способ - включить его в определение модели с помощью слоев 
предварительной обработки Keras. Мы воспользуемся этим подходом. Основным преимуществом для нас является то, что 
преобразования изображений будут вычисляться на GPU, а не на CPU, что потенциально ускоряет обучение.   

В этом упражнении мы узнаем, как улучшить классификатор из Урока 1 за счет увеличения данных. Следующая скрытая 
ячейка устанавливает конвейер данных. 

```python
# Imports
import os, warnings
import matplotlib.pyplot as plt
from matplotlib import gridspec

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory

# Reproducability
def set_seed(seed=31415):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
set_seed()

# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells


# Load training and validation sets
ds_train_ = image_dataset_from_directory(
    '../input/car-or-truck/train',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=True,
)
ds_valid_ = image_dataset_from_directory(
    '../input/car-or-truck/valid',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=False,
)

# Data Pipeline
def convert_to_float(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE
ds_train = (
    ds_train_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
ds_valid = (
    ds_valid_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
```
### Шаг 2 - Определите модель
Чтобы проиллюстрировать эффект аугментации, мы просто добавим пару простых преобразований к модели из Урока 1.

```python
from tensorflow import keras
from tensorflow.keras import layers
# these are a new feature in TF 2.2
from tensorflow.keras.layers.experimental import preprocessing


pretrained_base = tf.keras.models.load_model(
    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False

model = keras.Sequential([
    # Preprocessing
    preprocessing.RandomFlip('horizontal'), # flip left-to-right
    preprocessing.RandomContrast(0.5), # contrast change by up to 50%
    # Base
    pretrained_base,
    # Head
    layers.Flatten(),
    layers.Dense(6, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])
```
### Шаг 3 - Тренируйте и оценивайте¶
А теперь приступим к обучению! 

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'],
)

history = model.fit(
    ds_train,
    validation_data=ds_valid,
    epochs=30,
    verbose=0,
)
```


```python
import pandas as pd

history_frame = pd.DataFrame(history.history)

history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
```

Кривые обучения и проверки в модели из Урока 1 довольно быстро разошлись, предполагая, что она может выиграть от 
некоторой регуляризации. Кривые обучения для этой модели смогли остаться ближе друг к другу, и мы достигли 
некоторого умеренного улучшения в потерях при проверке и точности. Это говорит о том, что набор данных действительно 
выиграл от увеличения.   
