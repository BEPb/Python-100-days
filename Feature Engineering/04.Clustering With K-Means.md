Clustering With K-Means
### Вступление
В этом и следующем уроках используются так называемые алгоритмы обучения без учителя. Алгоритмы без учителя не 
используют цель; вместо этого их цель - изучить некоторые свойства данных, чтобы определенным образом представить 
структуру функций. В контексте проектирования признаков для прогнозирования, вы можете думать о неконтролируемом 
алгоритме как о методе «обнаружения признаков».

Кластеризация просто означает присвоение точек данных группам в зависимости от того, насколько они похожи друг на 
друга. Алгоритм кластеризации заставляет, так сказать, «птичьих перьев».

При использовании для разработки функций мы могли бы попытаться обнаружить группы клиентов, представляющих, 
например, рыночный сегмент или географические районы, которые имеют похожие погодные условия. Добавление функции 
меток кластера может помочь моделям машинного обучения распутать сложные отношения пространства или близости.

###Метки кластера как функция
Применительно к единственному объекту с действительным знаком кластеризация действует как традиционное 
преобразование «биннинг» или «дискретизация». Для нескольких функций это похоже на «многомерное бинирование» 
(иногда называемое векторным квантованием).

Слева: кластеризация одного объекта. Справа: кластеризация по двум функциям.
Добавленная к фрейму данных функция меток кластера может выглядеть следующим образом:

Кластер долготы и широты
-93,619 42,054 3
-93,619 42,053 3
-93,638 42,060 1
-93,602 41,988 0

Важно помнить, что эта функция кластера категорична. Здесь он показан с кодировкой метки (т. Е. Как 
последовательность целых чисел), которую может произвести типичный алгоритм кластеризации; в зависимости от вашей 
модели более подходящим может оказаться быстрое кодирование.

Идея добавления меток кластера заключается в том, что кластеры будут разбивать сложные отношения между функциями на 
более простые фрагменты. Затем наша модель может просто изучать более простые фрагменты один за другим, вместо того,
чтобы изучать сложное целое сразу. Это стратегия «разделяй и властвуй».


Кластеризация функции YearBuilt помогает этой линейной модели изучить ее связь с SalePrice.
На рисунке показано, как кластеризация может улучшить простую линейную модель. Изогнутая связь между YearBuilt и 
SalePrice слишком сложна для такой модели - она не подходит. Однако на меньших кусках связь почти линейна, и 
модель может легко учиться.

###Кластеризация k-средних
Алгоритмов кластеризации великое множество. Они различаются, прежде всего, тем, как они измеряют «сходство» или 
«близость» и с какими функциями они работают. Алгоритм, который мы будем использовать, k-means, интуитивно понятен 
и прост в применении в контексте разработки функций. В зависимости от вашего приложения другой алгоритм может быть 
более подходящим.

К-средство кластеризации измеряет сходство, используя обычное расстояние по прямой (другими словами, евклидово 
расстояние). Он создает кластеры, помещая несколько точек, называемых центроидами, внутри пространственного объекта.
Каждая точка в наборе данных назначается кластеру ближайшего к ней центроида. «K» в «k-средних» означает, сколько 
центроидов (то есть кластеров) он создает. Вы сами определяете k.

Вы можете представить, как каждый центроид захватывает точки через последовательность излучающих кругов. Когда 
наборы кругов из конкурирующих центроидов перекрываются, они образуют линию. В результате получилось то, что 
называется разоблачением Вороного. Сообщение показывает, каким кластерам будут назначены будущие данные; 
tessallation - это, по сути, то, что k-means извлекает из обучающих данных.

Кластеризация в наборе данных Эймса выше - это кластеризация k-средних. Вот тот же рисунок с показанными 
тессаляциями и центроидами.


Кластеризация по методу K-средних создает представление пространства признаков по Вороному.
Давайте рассмотрим, как алгоритм k-средних изучает кластеры и что это означает для проектирования функций. Мы 
сосредоточимся на трех параметрах из реализации scikit-learn: n_clusters, max_iter и n_init.

Это простой двухэтапный процесс. Алгоритм начинается со случайной инициализации некоторого предопределенного числа 
(n_clusters) центроидов. Затем он выполняет итерацию этих двух операций:

- назначить точки ближайшему центроиду кластера
- переместите каждый центроид, чтобы минимизировать расстояние до его точек


Он повторяет эти два шага до тех пор, пока центроиды не перестанут двигаться или пока не пройдет некоторое 
 максимальное количество итераций (max_iter).

Часто случается, что начальное случайное положение центроидов заканчивается плохой кластеризацией. По этой причине 
алгоритм повторяется несколько раз (n_init) и возвращает кластеризацию, которая имеет наименьшее общее расстояние 
между каждой точкой и ее центроидом, оптимальную кластеризацию.

На анимации ниже показан алгоритм в действии. Он иллюстрирует зависимость результата от начальных центроидов и 
 важность повторения до сходимости.


###Алгоритм кластеризации K-средних при аренде жилья Airbnb в Нью-Йорке.
Возможно, вам потребуется увеличить max_iter для большого количества кластеров или n_init для сложного набора 
данных. Обычно единственный параметр, который вам нужно выбрать самостоятельноэто n_clusters (то есть k). Лучшее 
разбиение для набора функций зависит от модели, которую вы используете, и от того, что вы пытаетесь предсказать, 
поэтому лучше всего настроить ее, как любой гиперпараметр (например, посредством перекрестной проверки).

###Пример - California Housing
В качестве пространственных характеристик «Широта» и «Долгота» California Housing являются естественными 
кандидатами для кластеризации k-средних. В этом примере мы сгруппируем их с MedInc (средний доход), чтобы создать 
экономические сегменты в разных регионах Калифорнии.

```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans

plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

df = pd.read_csv("../input/fe-course-data/housing.csv")
X = df.loc[:, ["MedInc", "Latitude", "Longitude"]]
X.head()
```

MedInc Широта Долгота
0 8,3252 37,88 -122,23
1 8,3014 37,86 -122,22
2 7,2574 37,85 -122,24
3 5,6431 37,85 -122,25
4 3,8462 37,85 -122,25


Поскольку кластеризация k-средних чувствительна к масштабированию, рекомендуется изменить масштаб или нормализовать 
данные с помощью экстремальных значений. Наши функции уже находятся примерно в том же масштабе, поэтому мы оставим 
их как есть.

# Создать кластерную функцию
```python
# Create cluster feature
kmeans = KMeans(n_clusters=6)
X["Cluster"] = kmeans.fit_predict(X)
X["Cluster"] = X["Cluster"].astype("category")

X.head()
```
Кластер широты и долготы MedInc
0 8,3252 37,88 -122,23 4
1 8,3014 37,86 -122,22 4
2 7,2574 37,85 -122,24 4
3 5,6431 37,85 -122,25 4
4 3,8462 37,85 -122,25 0

Теперь давайте посмотрим на пару графиков, чтобы увидеть, насколько это было эффективно. Во-первых, диаграмма 
рассеяния, показывающая географическое распределение кластеров. Похоже, что алгоритм создал отдельные сегменты для 
прибрежных районов с более высокими доходами.

```python
sns.relplot(
    x="Longitude", y="Latitude", hue="Cluster", data=X, height=6,
)
```

Целевым объектом в этом наборе данных является MedHouseVal (медианная стоимость дома). Эти коробчатые диаграммы 
показывают распределение цели в каждом кластере. Если кластеризация носит информативный характер, эти распределения 
должны по большей части разделяться в MedHouseVal, что мы и видим.

```python
X["MedHouseVal"] = df["MedHouseVal"]
sns.catplot(x="MedHouseVal", y="Cluster", data=X, kind="boxen", height=6);
```