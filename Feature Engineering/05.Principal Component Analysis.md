Principal Component Analysis
###Вступление
В предыдущем уроке мы рассмотрели наш первый основанный на модели метод проектирования функций: кластеризацию. В 
этом уроке мы рассмотрим следующий этап: анализ главных компонентов (PCA). Подобно тому, как кластеризация - это 
разделение набора данных на основе близости, вы можете рассматривать PCA как разделение вариации данных. PCA - 
отличный инструмент, который поможет вам обнаружить важные взаимосвязи в данных, а также может использоваться для 
создания более информативных функций.

(Техническое примечание: PCA обычно применяется к стандартизированным данным. Для стандартизованных данных 
«вариация» означает «корреляцию». Для нестандартных данных «вариация» означает «ковариацию». Все данные в этом курсе будут стандартизированы перед применением PCA.)

###Анализ главных компонентов
В наборе данных Abalone есть физические измерения, сделанные с нескольких тысяч тасманских морских ушек. (Морское 
морское ушко очень похоже на моллюска или устрицу.) А пока мы просто рассмотрим пару характеристик: «Высота» и 
«Диаметр» их раковин.

Вы можете представить себе, что в этих данных есть «оси вариации», которые описывают, чем морское ушко имеет 
тенденцию отличаться друг от друга. Графически эти оси выглядят как перпендикулярные линии, проходящие вдоль 
естественных размеров данных, по одной оси для каждого исходного объекта.


Часто мы можем дать имена этим осям вариации. Более длинную ось мы могли бы назвать компонентом «Размер»: малая 
высота и малый диаметр (внизу слева) в отличие от большой высоты и большого диаметра (вверху справа). Более 
короткую ось мы могли бы назвать компонентом «Форма»: небольшая высота и большой диаметр (плоская форма) в отличие 
от большой высоты и малого диаметра (круглая форма).

Обратите внимание, что вместо того, чтобы описывать морские ушки по их «высоте» и «диаметру», мы могли бы точно так 
же описывать их по их «размеру» и «форме». Фактически, в этом и заключается вся идея PCA: вместо описания данных с 
помощью исходных функций мы описываем их с помощью осей вариации. Оси вариации становятся новыми функциями.


Основные компоненты становятся новыми объектами за счет вращения набора данных в пространстве признаков.
Конструкции PCA новых функций на самом деле представляют собой просто линейные комбинации (взвешенные суммы) 
исходных функций:

df ["Размер"] = 0,707 * X ["Высота"] + 0,707 * X ["Диаметр"]
df ["Форма"] = 0,707 * X ["Высота"] - 0,707 * X ["Диаметр"]

Эти новые функции называются основными компонентами данных. Сами веса называются нагрузками. Основных компонентов 
будет столько, сколько функций в исходном наборе данных: если бы мы использовали десять функций вместо двух, у нас 
было бы десять компонентов.

Нагрузки компонента говорят нам, какие вариации он выражает через знаки и величины:

Характеристики \ Компоненты Размер (ПК1) Форма (ПК2)
Высота 0,707 0,707
Диаметр 0,707 -0,707

Эта таблица нагрузок говорит нам, что в компоненте «Размер» высота и диаметр изменяются в одном направлении (тот же 
знак), но в компоненте «Форма» они меняются в противоположных направлениях (противоположный знак). В каждом 
компоненте нагрузки имеют одинаковую величину, поэтому функции одинаково влияют на оба компонента.

PCA также сообщает нам количество вариаций в каждом компоненте. Из рисунков видно, что данные по компоненту Size 
больше, чем по компоненту Shape. PCA делает это точным с помощью процента объясненной дисперсии каждого компонента.


Размер составляет около 96%, а форма - около 4% разницы между высотой и диаметром.
Компонент "Размер" улавливает большую часть различий между высотой и диаметром. Однако важно помнить, что величина 
дисперсии в компоненте не обязательно соответствует тому, насколько он хорош как предсказатель: это зависит от того,
что вы пытаетесь предсказать.

PCA для проектирования функций
Есть два способа использовать PCA для разработки функций.

Первый способ - использовать его как описательную технику. Поскольку компоненты сообщают вам об вариации, вы можете 
вычислить показатели MI для компонентов и посмотреть, какой вид вариации наиболее предсказуем для вашей цели. Это 
могло бы дать вам идеи для типов элементов, которые нужно создать - произведение «Высота» и «Диаметр», если, скажем,
«Размер», или соотношение «Высота» и «Диаметр», если важна форма. Вы даже можете попробовать кластеризацию по 
одному или нескольким компонентам с высокими показателями.

Второй способ - использовать сами компоненты как функции. Поскольку компоненты напрямую раскрывают вариационную 
структуру данных, они часто могут быть более информативными, чем исходные функции. Вот несколько вариантов 
использования:

Снижение размерности: когда ваши функции сильно избыточны (в частности, мультиколлинеарны), PCA разделит 
избыточность на один или несколько компонентов с близкой к нулю дисперсией, которые вы затем можете отбросить, 
поскольку они будут содержать мало или совсем не содержать информации.
Обнаружение аномалий: необычные вариации, не очевидные для исходных функций, часто обнаруживаются в компонентах с 
низкой дисперсией. Эти компоненты могут быть очень информативными в случае аномалии или выброса.задача обнаружения.
Подавление шума: набор показаний датчика часто имеет общий фоновый шум. PCA может иногда собирать (информативный) 
сигнал в меньшее количество функций, оставляя шум в покое, тем самым повышая отношение сигнал / шум.
Декорреляция: некоторые алгоритмы машинного обучения борются с сильно коррелированными функциями. PCA преобразует 
коррелированные функции в некоррелированные компоненты, с которыми вашему алгоритму может быть проще работать.
PCA в основном дает вам прямой доступ к корреляционной структуре ваших данных. Вы, несомненно, придумаете 
собственные приложения!

###Лучшие практики PCA
При применении PCA следует помнить о нескольких вещах:
PCA работает только с числовыми функциями, такими как непрерывные количества или счетчики.
PCA чувствителен к масштабированию. Перед применением PCA рекомендуется стандартизировать данные, если только вы не 
знаете, что у вас есть веские причины не делать этого.
Рассмотрите возможность удаления или ограничения выбросов, поскольку они могут оказать ненадлежащее влияние на 
результаты.

###Пример - Автомобили 1985 года.
В этом примере мы вернемся к нашему набору данных Automobile и применим PCA, используя его в качестве описательной 
техники для обнаружения функций. В упражнении мы рассмотрим другие варианты использования.

Эта скрытая ячейка загружает данные и определяет функции plot_variance и make_mi_scores.

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from IPython.display import display
from sklearn.feature_selection import mutual_info_regression


plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)


def plot_variance(pca, width=8, dpi=100):
    # Create figure
    fig, axs = plt.subplots(1, 2)
    n = pca.n_components_
    grid = np.arange(1, n + 1)
    # Explained variance
    evr = pca.explained_variance_ratio_
    axs[0].bar(grid, evr)
    axs[0].set(
        xlabel="Component", title="% Explained Variance", ylim=(0.0, 1.0)
    )
    # Cumulative Variance
    cv = np.cumsum(evr)
    axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")
    axs[1].set(
        xlabel="Component", title="% Cumulative Variance", ylim=(0.0, 1.0)
    )
    # Set up figure
    fig.set(figwidth=8, dpi=100)
    return axs

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores


df = pd.read_csv("../input/fe-course-data/autos.csv")
```

Мы выбрали четыре функции, которые охватывают ряд свойств. Каждая из этих функций также имеет высокий показатель MI 
с целевой ценой. Мы стандартизируем данные, поскольку эти функции, естественно, не в одном масштабе.

```python
features = ["highway_mpg", "engine_size", "horsepower", "curb_weight"]

X = df.copy()
y = X.pop('price')
X = X.loc[:, features]

# Standardize
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
```

Теперь мы можем установить оценщик PCA scikit-learn и создать основные компоненты. Здесь вы можете увидеть первые 
несколько строк преобразованного набора данных.

```python
from sklearn.decomposition import PCA

# Create principal components
# Создать основные компоненты
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Convert to dataframe
# Преобразовать в фрейм данных
component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns=component_names)

X_pca.head()
```


X_pca.head ()
ПК1 ПК2 ПК3 ПК4
0 0,382486 -0,400222 0,124122 0,169539
1 0,382486 -0,400222 0,124122 0,169539
2 1,550890 -0,107175 0,598361 -0,256081
3 -0,408859 -0,425947 0,243335 0,013920
4 1,132749 -0,814565 -0,202885 0,224138


После подгонки экземпляр PCA содержит нагрузки в своем атрибуте components_. (Терминология PCA, к сожалению, 
непоследовательна. Мы следуем соглашению, согласно которому преобразованные столбцы в X_pca называются компонентами,
которые в противном случае не имеют имени.) Мы заключим загрузки в фрейм данных.


```python
loadings = pd.DataFrame(
    pca.components_.T,  # transpose the matrix of loadings
    columns=component_names,  # so the columns are the principal components
    index=X.columns,  # and the rows are the original features
)
loadings
```
загрузки
ПК1 ПК2 ПК3 ПК4
Highway_mpg -0,492347 0,770892 0,070142 -0,397996
размер_двигателя 0,503859 0,626709 0,019960 0,594107
мощность 0,500448 0,013788 0,731093 -0,463534
curb_weight 0.503262 0.113008 -0.678369 -0.523232


Напомним, что знаки и величины нагрузок на компоненты говорят нам, какие вариации фиксируются. Первый компонент 
(PC1) показывает контраст между большими, мощными автомобилями с плохим пробегом по бензину и меньшими, более 
экономичными автомобилями с хорошим пробегом по бензину. Мы могли бы назвать это осью «Роскошь / Экономика». На 
следующем рисунке показано, что выбранные нами четыре функции в основном различаются по оси Luxury / Economy.

# Посмотрите на объясненную дисперсию
```python
# Look at explained variance
plot_variance(pca);
```

Давайте также посмотрим на показатели MI компонентов. Неудивительно, что PC1 очень информативен, хотя остальные 
компоненты Несмотря на их небольшую вариативность, они все еще имеют существенную связь с ценой. Изучение этих 
компонентов может оказаться полезным, чтобы найти взаимосвязи, не охватываемые основной осью роскоши / экономики.

```python
mi_scores = make_mi_scores(X_pca, y, discrete_features=False)
mi_scores
```
ПК1 1.013800
ПК2 0,379440
PC3 0,306502
PC4 0.204447
Имя: MI Scores, dtype: float64

Третий компонент показывает контраст между мощностью и снаряженным весом - кажется, спортивные автомобили и универсалы.
```python
# Show dataframe sorted by PC3
# Показать фрейм данных, отсортированный по ПК3
idx = X_pca["PC3"].sort_values(ascending=False).index
cols = ["make", "body_style", "horsepower", "curb_weight"]
df.loc[idx, cols]
```

сделать body_style лошадиные силы curb_weight
118 порше хардтоп 207 2756
117 порше хардтоп 207 2756
119 порше кабриолет 207 2800
45 ягуар седан 262 3950
96 ниссан хэтчбек 200 3139
... ... ... ... ...
59 mercedes-benz универсал 123 3750
61 mercedes-benz седан 123 3770
101 Peugot универсал 95 3430
105 Peugot универсал 95 3485
143 Тойота универсал 62 3110
193 строки × 4 столбца

Чтобы выразить этот контраст, давайте создадим новую функцию соотношения:

```python
df["sports_or_wagon"] = X.curb_weight / X.horsepower
sns.regplot(x="sports_or_wagon", y='price', data=df, order=2);
```