### Обработка данных
В этом эксперименте мы используем набор данных refmoves1k_kaggle в качестве набора данных оценки модели. Этот набор 
данных представляет собой набор данных самопроверки, созданный Питером Кнудде для конкурса Connect X на платформе 
Kaggle. Набор данных содержит 1000 лиц доски. , каждый из которых представляет собой соответствующий счет (счет) и 
соответствующий счет (счет за ход) в каждом столбце под доской. В качестве примера возьмем фрагмент данных:   

{"доска": [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1, 
1, 1, 2, 1, 0, 1, 2], «счет»: -2, «ход оценка ": [-3, -4, -4, -2, -6, -5, -4]} 

- board: Шахматная поверхность, хранящаяся в виде одномерного массива;
- score: Лучший результат на текущей доске;
- move score: Одномерный массив, содержащий семь значений, каждое значение представляет счет шахматной фигуры, 
помещенной в этот столбец. Конкретное значение счета: 

- Score = 0: Игра будет вничью;
- Score > 0: Теперь игрок, держащий шахматы, выиграет, чем выше значение очков, тем быстрее будет победа. В игре у 
  каждого игрока есть 21 фигура, если score = +5, то это означает, что игрок (21 - 5) * 2 = 32определит победу на первом шахматном ходу;
- Score < 0: Теперь игрок, держащий шахматы, проиграет игру.Чем больше абсолютное значение счета, тем быстрее игра 
  будет проиграна;
- Score = -99： Это означает, что на данной позиции нельзя играть в шахматы.
Приведите более наглядный пример:

{"доска": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 
 0, 1, 2, 1, 1, 2, 0], «счет»: 0, «счет хода ": [-16, -16, 0, -16, -16, -16, -16]}

модель используется→ Алгоритм→Агент конструирует агента:

Модель: 
Модель используется для определения прямой сети. В этом эксперименте прямая сеть состоит из 
четырех сверточных слоев и четырех полностью связанных слоев. Последние два полностью связанных слоя объединены с 
функциями активации для вывода вероятностей стратегии соответственно. Число Пи(s,а)\пи(s,а) π(s,а). И ценность v(s)
v(s)v(s)。   

Алгоритм: 
алгоритм определяет конкретный алгоритм обновления прямой сети (модели), то есть обновления модели путем 
определения функции потерь. Вычисления, относящиеся к алгоритмам, могут быть определены в алгоритме. 

Агент: 
Агент отвечает за взаимодействие между алгоритмом и средой и предоставляет сгенерированные данные алгоритму 
для обновления модели во время взаимодействия. 

### Поиск дерева Монте-Карло на основе нейронной сети
Для каждого узла в дереве Монте-Карло он содержит следующие переменные:

Qsa: Ожидаемая награда за выполнение действия в состоянии s;
Nsa: Количество посещений края (s, a) в дереве поиска;
Ns: Количество посещений состояния доски;
Ps: Вероятность действия предсказывается нейронной сетью, то есть начальное значение вероятности политики;
Es: Состояние игры, соответствующее состоянию доски s, выигрыш, проигрыш, ничья или игра не окончена;
Vs: Возможное действие, соответствующее состоянию платы s
В [ ]
EPS = 1e-8

Актер эквивалентен игроку в игре, и его основные функции заключаются в следующем:

- self_play: Используйте MCTS для самостоятельной игры и сбора данных;
- pitting: MCTS, построенная нейронной сетью предыдущего поколения, противопоставляется MCTS, построенной нейронной 
  сетью нового поколения, и недавно обученная модель нейронной сети принимается, когда определяется окончательная победа;
- evaluate_test_dataset: Используйте данные refmoves1k_kaggle для оценки модели, индикаторы оценки - Perfect Move и 
   Good Move. Среди них Perfect Move означает, что агент выбрал действие, которое увеличивает количество очков, а 
   Good Move означает, что агент выбрал тот же тип действия, что и идеальный ход. Хотя это не оптимальное действие, 
   оно все же относится к категории того же типа, что и идеальный ход. Например, он также может сделать игрока. 
   Победа, поражение или ничья;
- _executeEpisode: Выполнить эпизод до конца игры.


### Тренер используется для управления обучением агента. Каждый раунд обучения разделен на четыре этапа:

1. самостоятельная игра: самостоятельная игра, увеличение количества данных;
2. узнать: обучающая нейронная сеть;
3. оценить_test_dataset: оценить недавно обученную сеть;
4. Питтинг: старая сеть борется с новой сетью и решает, следует ли использовать новую обученную сеть, в соответствии с 
   коэффициентом выигрыша.

поскольку модель будет сохраняться, обучаться и оцениваться один раз для каждой итерации, они вызываются шаг за 
шагом через класс Coach. 