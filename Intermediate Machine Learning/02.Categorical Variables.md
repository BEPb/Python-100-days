###Categorical Variables
В этом руководстве вы узнаете, что такое категориальная переменная, а также три подхода к обработке этого типа данных.

###Вступление
Категориальная переменная принимает только ограниченное количество значений.

 Рассмотрим опрос, который спрашивает, как часто вы завтракаете, и предлагает четыре варианта: «Никогда», «Редко», 
  «Большую часть дней» или «Каждый день». В этом случае данные являются категориальными, потому что ответы попадают 
 в фиксированный набор категорий.
 Если бы люди ответили на опрос о том, какой маркой автомобилей они владеют, ответы распределились бы по таким 
 категориям, как «Хонда», «Тойота» и «Форд». В этом случае данные также категоричны.
 Вы получите сообщение об ошибке, если попытаетесь подключить эти переменные к большинству моделей машинного 
  обучения в Python без предварительной их обработки. В этом руководстве мы сравним три подхода, которые вы можете 
 использовать для подготовки категориальных данных.

Три подхода
###1) Отбросьте категориальные переменные
 Самый простой подход к работе с категориальными переменными - просто удалить их из набора данных. Этот подход будет 
 работать только в том случае, если столбцы не содержат полезной информации.

###2) Порядковое кодирование
Порядковое кодирование присваивает каждому уникальному значению отдельное целое число.
Этот подход предполагает упорядочение категорий: «Никогда» (0) <«Редко» (1) <«Большинство дней» (2) <«Каждый день» (3).

Это предположение имеет смысл в этом примере, потому что существует бесспорное ранжирование категорий. Не все 
 категориальные переменные имеют четкий порядок значений, но мы называем те, которые имеют, как порядковые 
переменные. Для моделей на основе деревьев (таких как деревья решений и случайные леса) можно ожидать, что 
порядковое кодирование будет хорошо работать с порядковыми переменными.

###3) Горячее кодирование
 ри однократном кодировании создаются новые столбцы, указывающие наличие (или отсутствие) каждого возможного 
 значения в исходных данных. Чтобы понять это, мы рассмотрим пример. 
 В исходном наборе данных «Цвет» - это категориальная переменная с тремя категориями: «Красный», «Желтый» и 
 «Зеленый». Соответствующая горячая кодировка содержит один столбец для каждого возможного значения и одну строку 
 для каждой строки в исходном наборе данных. Если исходным значением было «Красный», мы помещаем 1 в столбец 
 «Красный»; если исходным значением было «Желтый», мы помещаем 1 в столбец «Желтый» и так далее.

В отличие от порядкового кодирования, горячее кодирование не предполагает упорядочения категорий. Таким образом, вы 
можете ожидать, что этот подход будет работать особенно хорошо, если в категориальных данных нет четкого 
упорядочения (например, «Красный» не больше и не меньше, чем «Желтый»). Мы называем категориальные переменные без внутреннего ранжирования номинальными переменными.

Одноразовое кодирование обычно не работает хорошо, если категориальная переменная принимает большое количество значений (т.е. вы обычно не будете использовать его для переменных, принимающих более 15 различных значений).

Пример
Как и в предыдущем руководстве, мы будем работать с набором данных Melbourne Housing.

Мы не будем заострять внимание  на этапе загрузки данных. Вместо этого вы можете представить, что находитесь в точке,
где у вас уже есть данные для обучения и проверки в X_train, X_valid, y_train и y_valid.
```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Separate target from predictors
y = data.Price
X = data.drop(['Price'], axis=1)

# Divide data into training and validation subsets
X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                                random_state=0)

# Drop columns with missing values (simplest approach)
cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] 
X_train_full.drop(cols_with_missing, axis=1, inplace=True)
X_valid_full.drop(cols_with_missing, axis=1, inplace=True)

# "Cardinality" means the number of unique values in a column
# Select categorical columns with relatively low cardinality (convenient but arbitrary)
low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and 
                        X_train_full[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]

# Keep selected columns only
my_cols = low_cardinality_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
```
Мы взглянем на данные обучения с помощью метода head () ниже.

```X_train.head ()```

Затем мы получаем список всех категориальных переменных в обучающих данных.

Мы делаем это, проверяя тип данных (или dtype) каждого столбца. Объект dtype указывает, что в столбце есть текст 
(теоретически это может быть и другое, но для наших целей это неважно). Для этого набора данных столбцы с текстом 
указывают категориальные переменные.

```python
# Получить список категориальных переменных
# Get list of categorical variables
s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables:")
print(object_cols)
```

Категориальные переменные:
["Тип", "Метод", "Название региона"]

####Определите функцию для измерения качества каждого подхода
Мы  определяем функцию score_dataset () для сравнения трех различных подходов к работе с категориальными переменными.
Эта  функция сообщает среднюю абсолютную ошибку (MAE) для модели случайного леса. В общем, мы хотим, чтобы MAE была 
как можно ниже!


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# Function for comparing different approaches
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=100, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)
```



####Оценка по подходу 1 (исключение категориальных переменных)
Мы отбрасываем столбцы объекта с помощью метода select_dtypes ().

```python
drop_X_train = X_train.select_dtypes(exclude=['object'])
drop_X_valid = X_valid.select_dtypes(exclude=['object'])

print("MAE from Approach 1 (Drop categorical variables):")
print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))
```
MAE из подхода 1 (исключить категориальные переменные):
175703.4818515

####Оценка по подходу 2 (порядковое кодирование)
Scikit-learn имеет класс OrdinalEncoder, который можно использовать для получения порядковых кодировок. Мы 
перебираем категориальные переменные и применяем порядковый кодировщик отдельно к каждому столбцу.


```python
from sklearn.preprocessing import OrdinalEncoder

# Make copy to avoid changing original data 
# Сделать копию, чтобы избежать изменения исходных данных
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# Apply ordinal encoder to each column with categorical data
# Применить порядковый кодировщик к каждому столбцу с категориальными данными
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

print("MAE from Approach 2 (Ordinal Encoding):") 
print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))
```
MAE из подхода 2 (порядковое кодирование):
165936.40548390493

В приведенной выше ячейке кода для каждого столбца мы случайным образом присваиваем каждому уникальному значению 
разное целое число. Это распространенный подход, который проще, чем создание настраиваемых меток; однако мы можем 
ожидать дополнительного повышения производительности, если предоставим более информированные метки для всех 
порядковых переменных.

### Оценка по подходу 3 (одноразовое кодирование)
Мы используем класс OneHotEncoder из scikit-learn, чтобы получить горячие кодировки. Есть ряд параметров, которые 
можно использовать для настройки его поведения.

Мы устанавливаем handle_unknown = 'ignore', чтобы избежать ошибок, когда данные проверки содержат классы, которые 
не представлены в данных обучения, и
установка sparse = False гарантирует, что закодированные столбцы будут возвращены в виде массива numpy (вместо 
разреженной матрицы).
Чтобы использовать кодировщик, мы предоставляем только категориальные столбцы, которые мы хотим закодировать в 
горячем режиме. Например, для кодирования обучающих данных мы предоставляем X_train [object_cols]. (object_cols в 
ячейке кода ниже представляет собой список имен столбцов с категориальными данными, поэтому X_train [object_cols] 
содержит все категориальные данные в обучающем наборе.)

```python
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
# Применяем быстрый энкодер к каждому столбцу с категориальными данными
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
# One-hot кодирование удалено index; положить его обратно
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
# Удалить категориальные столбцы (заменим на горячую кодировку)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
# Добавить столбцы с горячим кодированием к числовым функциям
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

print("MAE from Approach 3 (One-Hot Encoding):") 
print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))
```
MAE из подхода 3 (горячее кодирование):
166089.4893009678


Какой подход лучше?
В этом случае отбрасывание категориальных столбцов (подход 1) показало худший результат, поскольку у него был самый 
высокий балл MAE. Что касается двух других подходов, поскольку возвращенные оценки MAE настолько близки по значению,
похоже, нет какого-либо значимого преимущества одного по сравнению с другим.

В общем, однократное кодирование (подход 3) обычно работает лучше всего, а отбрасывание категориальных столбцов 
(подход 1) обычно работает хуже, но это зависит от конкретного случая.

Заключение
Мир наполнен категориальными данными. Вы станете гораздо более эффективным специалистом по данным, если будете 
знать, как использовать этот общий тип данных!

