# AI Fairness

### Введение
Есть много разных способов определить, что мы можем искать в модели справедливого машинного обучения (ML). Например, 
предположим, что мы работаем с моделью, которая одобряет (или отклоняет) заявки на кредитные карты. Это: 

- справедливо, если уровень одобрения одинаков для мужчин и женщин, или
- лучше, если пол будет удален из набора данных и скрыт от модели?


В этом уроке мы рассмотрим четыре критерия, которые мы можем использовать, чтобы решить, является ли модель 
  справедливой. Затем вы примените полученные знания в практическом упражнении, где запустите код для обучения 
  нескольких моделей и проанализируете справедливость каждой из них. (Что касается каждого урока в этом курсе, 
  предварительный опыт программирования не требуется.)   

### Четыре критерия справедливости
Эти четыре критерия справедливости являются полезной отправной точкой, но важно отметить, что существуют и другие 
способы формализации справедливости, которые вам предлагается изучить. 

Предположим, мы работаем с моделью, которая выбирает людей для получения определенного результата. Например, модель 
может выбирать людей, которые должны быть одобрены для кредита, приняты в университет или предложены возможности 
работы. (Поэтому мы не рассматриваем модели, выполняющие, среди прочего, такие задачи, как распознавание лиц или 
генерация текста.)   

1. Демографический паритет/статистический паритет
Демографический паритет говорит о том, что модель справедлива, если состав людей, выбранных моделью, соответствует 
   проценту членства в группе претендентов. 

Некоммерческая организация организует международную конференцию, на участие в которой записались 20 000 человек. 
Организаторы пишут модель машинного обучения, чтобы выбрать 100 участников, которые потенциально могут выступить с 
интересными докладами на конференции. Поскольку 50% участников будут женщины (10 000 из 20 000), они разработали 
модель таким образом, чтобы 50% выбранных кандидатов-спикеров были женщинами.    

2. Равные возможности
Справедливость равных возможностей гарантирует, что доля людей, которые должны быть выбраны моделью («положительные»)
   , которые правильно выбраны моделью, одинакова для каждой группы. Мы называем эту долю истинной положительной 
   скоростью (TPR) или чувствительностью модели.  

Врач использует инструмент для выявления пациентов, нуждающихся в дополнительной помощи, которые могут подвергаться 
риску развития серьезных заболеваний. (Этот инструмент используется только в дополнение к практике врача, как второе 
мнение.) Он предназначен для обеспечения высокого TPR, одинакового для каждой демографической группы.  

3. Одинаковая точность
В качестве альтернативы мы могли бы проверить, что модель имеет одинаковую точность для каждой группы. То есть 
   процент правильных классификаций (люди, которым следует отказать и которым отказывают, и люди, которые должны 
   быть одобрены, которые одобрены) должны быть одинаковыми для каждой группы. Если модель имеет точность 98% для 
   отдельных лиц в одной группе, она должна быть точной на 98% для других групп.   

Банк использует модель для утверждения кредита. Модель спроектирована так, чтобы быть одинаково точной для каждой 
демографической группы: таким образом, банк избегает утверждения людей, которые должны быть отклонены (что нанесет 
финансовый ущерб как заявителю, так и банку), и не отклоняет людей, которые должны быть одобрены (что быть неудачной 
возможностью для заявителя и уменьшить доход банка).   

4. Групповое неосознавание / «Справедливость через неосознанность»
Групповая беспристрастность удаляет всю информацию о членстве в группе из набора данных. Например, мы можем удалить 
   гендерные данные, чтобы попытаться сделать модель справедливой для разных гендерных групп. Точно так же мы можем 
   удалить информацию о расе или возрасте.  

Одна из трудностей применения этого подхода на практике заключается в том, что нужно быть осторожным при 
идентификации и удалении прокси для данных о членстве в группе. Например, в городах с расовой сегрегацией почтовый 
индекс является надежным показателем расы. То есть, когда данные о расе удаляются, данные о почтовом индексе также 
должны быть удалены, иначе приложение машинного обучения все еще сможет вывести расу человека из данных. Кроме того, 
групповая неосознанная справедливость вряд ли будет хорошим решением для исторической предвзятости.     

Пример
Мы будем работать с небольшим примером, чтобы проиллюстрировать различия между четырьмя различными типами 
справедливости. Мы будем использовать матрицу путаницы, которая является распространенным инструментом, используемым 
для понимания производительности модели машинного обучения. Этот инструмент показан в приведенном ниже примере, 
который изображает модель с точностью 80% (поскольку 8/10 человек были правильно классифицированы) и имеет 83% 
истинно положительных результатов (поскольку 5/6 «положительных» результатов были правильно классифицированы).     



Чтобы понять, как производительность модели варьируется в зависимости от группы, мы можем построить другую матрицу 
путаницы для каждой группы. В этом небольшом примере мы предположим, что у нас есть данные только от 20 человек, 
поровну разделенных между двумя группами (10 из группы A и 10 из группы B).  

На следующем изображении показано, как могут выглядеть матрицы путаницы, если модель удовлетворяет требованиям 
справедливости демографического паритета. В модели учитывалось по 10 человек из каждой группы (50% из группы А и 50% 
из группы Б). Модель одобрила 14 человек, также поровну разделенных на группы (50% из группы А и 50% из группы Б).   



Для обеспечения равных возможностей TPR для каждой группы должно быть одинаковым; в приведенном ниже примере это 66% 
в каждом случае. 



Далее мы можем увидеть, как матрицы путаницы могут выглядеть для справедливости равной точности. Для каждой группы 
модель была точной на 80%. 



Обратите внимание, что групповая неосведомленная справедливость не может быть обнаружена с помощью матрицы путаницы 
и больше связана с удалением информации о членстве в группе из набора данных. 

Потратьте время на изучение этих игрушечных примеров и используйте их, чтобы развить свою интуицию в отношении 
различий между различными типами справедливости. Как изменится пример, если в группе А будет вдвое больше 
абитуриентов, чем в группе Б?  

Также обратите внимание, что ни один из примеров не удовлетворяет более чем одному типу справедливости. Например, 
пример демографического паритета не обеспечивает равной точности или равных возможностей. Потратьте время, чтобы 
проверить это сейчас. На практике невозможно оптимизировать модель для более чем одного типа справедливости: чтобы 
узнать больше об этом, изучите теорему невозможности машинной справедливости. Итак, какой критерий справедливости 
выбрать, если можно удовлетворить только одному? Как и в случае с большинством этических вопросов, правильный ответ 
обычно не однозначен, и выбор критерия должен стать долгим разговором с участием всех членов вашей команды.      

При работе с реальным проектом данных будет гораздо больше. В этом случае матрицы путаницы по-прежнему являются 
полезным инструментом для анализа производительности модели. Однако следует отметить одну важную вещь: обычно нельзя 
ожидать, что модели реального мира полностью удовлетворяют какому-либо определению справедливости. Например, если в 
качестве показателя справедливости выбран «демографический паритет», а цель модели состоит в том, чтобы выбрать 50% 
мужчин, может случиться так, что окончательная модель в конечном итоге выберет некоторый процент, близкий к 50%, но 
не точно ( как 48% или 53%).      

### Выучить больше
- Изучите различные типы справедливости с помощью интерактивного инструмента.
- Вы можете прочитать больше о равных возможностях в этой записи блога.
- Проанализируйте честность машинного обучения с помощью этого пошагового руководства по инструменту «что, если», 
  созданному командой исследований людей и ИИ (PAIR) в Google. Этот инструмент позволяет быстро изменить модель 
  машинного обучения после того, как вы выбрали критерий справедливости, который лучше всего подходит для вашего 
  варианта использования.  