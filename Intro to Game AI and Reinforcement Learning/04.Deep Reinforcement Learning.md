Deep Reinforcement Learning
### Глубокое обучение с подкреплением
Вступление
До сих пор наши агенты полагались на подробную информацию о том, как играть в игру. Эвристика действительно дает 
много советов о том, как выбирать ходы!

В этом руководстве вы узнаете, как использовать обучение с подкреплением для создания интеллектуального агента без 
использования эвристики. Вместо этого мы будем постепенно улучшать стратегию агента с течением времени, просто играя в игру и пытаясь максимизировать процент выигрышей.

В этой записной книжке мы не сможем подробно изучить эту сложную область, но вы узнаете общую картину и изучите код, который можно использовать для обучения собственного агента.

Нейронные сети
Трудно придумать идеальную эвристику. Улучшение эвристики обычно влечет за собой многократное прохождение игры, чтобы определить конкретные случаи, когда агент мог бы сделать лучший выбор. И это может оказаться сложной задачей - интерпретировать, что именно идет не так, и, в конечном итоге, исправить старые ошибки, не вводя случайно новые.

Разве не было бы намного проще, если бы у нас был более систематический способ улучшения агента с помощью игрового процесса?

В этом руководстве для достижения этой цели мы заменим эвристику нейронной сетью.

Сеть принимает текущую плату в качестве входных данных. И он выводит вероятность для каждого возможного хода.


Затем агент выбирает ход путем выборки из этих вероятностей. Например, для игрового поля на изображении выше агент выбирает столбец 4 с вероятностью 50%.

Таким образом, чтобы закодировать хорошую стратегию игрового процесса, нам нужно всего лишь изменить веса сети, чтобы для каждого возможного игрового поля она присваивала более высокие вероятности лучшим ходам.

По крайней мере теоретически, это наша цель. На практике мы не будем проверять, так ли это - ведь помните, что в Connect Four есть более 4 триллионов возможных игровых плат!

Настраивать
Как мы можем подойти к задаче изменения весов сети на практике? Вот подход, который мы будем использовать в этом уроке:

После каждого хода мы даем агенту награду, которая говорит ему, насколько хорошо он справился:
Если агент выигрывает игру этим ходом, мы даем ему награду +1.
В противном случае, если агент сделает неверный ход (который завершает игру), мы даем ему награду -10.
В противном случае, если противник выигрывает игру на следующем ходу (то есть агент не смог предотвратить победу своего оппонента), мы даем агенту вознаграждение в размере -1.
В противном случае агент получает награду 1/42.
В конце каждой игры агент суммирует свою награду. Мы называем сумму вознаграждения совокупным вознаграждением агента.
Например, если игра длилась 8 ходов (каждый игрок сыграл четыре раза), и в конечном итоге агент выиграл, то его совокупная награда составит 3 * (1/42) + 1.
Если игра длилась 11 ходов (и оппонент пошел первым, значит, агент сыграл пять раз), а оппонент выиграл последний ход, то совокупное вознаграждение агента составит 4 * (1/42) - 1.
Если игра заканчивается вничью, то агент выполняет ровно 21 ход и получает совокупную награду 21 * (1/42).
Если игра длилась 7 ходов и закончилась тем, что агент выбрал неверный ход, агент получает совокупное вознаграждение в размере 3 * (1/42) - 10.
Наша цель - найти веса нейронной сети, которые (в среднем) максимизируют совокупное вознаграждение агента.

Идея использования вознаграждения для отслеживания эффективности агента является ключевой идеей в области обучения с подкреплением. Определив проблему таким образом, мы можем использовать любой из множества алгоритмов обучения с подкреплением для создания агента.

Обучение с подкреплением
Существует множество различных алгоритмов обучения с подкреплением, среди которых DQN, A2C и PPO. Все эти алгоритмы используют аналогичный процесс для создания агента:

Первоначально веса устанавливаются на случайные значения.
Пока агент играет в игру, алгоритм постоянно пробует новые значения весов, чтобы увидеть, как в среднем влияет совокупное вознаграждение. Со временем, поиграв во многие игры, мы получаем хорошее представление о том, как веса влияют на совокупное вознаграждение, и алгоритм выбирает веса, которые работают лучше.
Конечно, здесь мы упустили детали, и в этом процессе есть много сложностей. А пока мы сосредотачиваемся на общей картине!
Таким образом, у нас получится агент, который пытается выиграть игру (поэтому он получает финальную награду +1 и избегает -1 и -10) и пытается продлить игру как можно дольше (так что что он собирает бонус 1/42 столько раз, сколько может).
Вы можете возразить, что на самом деле не имеет смысла желать, чтобы игра длилась как можно дольше - это может привести к очень неэффективному агенту, который не будет выполнять очевидные выигрышные ходы в начале игры. И ваша интуиция будет верна - из-за этого агенту потребуется больше времени, чтобы сделать выигрышный ход! Причина, по которой мы включаем бонус 1/42, заключается в том, чтобы помочь алгоритмам, которые мы будем использовать, лучше сходиться. Дальнейшее обсуждение выходит за рамки этого курса, но вы можете узнать больше, прочитав о «проблеме временного присвоения кредита» и «формировании вознаграждения».
В следующем разделе мы будем использовать алгоритм Proximal Policy Optimization (PPO) t


273 / 5000
Результаты перевода
o создать агента.

Код
В Интернете есть множество отличных реализаций алгоритмов обучения с подкреплением. В этом курсе мы будем использовать стабильные базовые показатели.

В настоящее время Stable Baselines еще несовместима с TensorFlow 2.0. Итак, мы начинаем с перехода на TensorFlow 1.0. 
```python
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
!pip install 'tensorflow==1.15.0'
# Check version of tensorflow
import tensorflow as tf
tf.__version__
```
Нам нужно проделать небольшую дополнительную работу, чтобы сделать среду совместимой со стабильными базовыми уровнями. Для этого мы определяем класс ConnectFourGym ниже. Этот класс реализует ConnectX как среду OpenAI Gym и использует несколько методов:

reset () будет вызываться в начале каждой игры. Он возвращает начальную игровую доску в виде двухмерного массива с 6 строками и 7 столбцами.
change_reward () настраивает вознаграждения, которые получает агент. (У конкурса уже есть собственная система вознаграждений, которые используются для ранжирования агентов, и этот метод изменяет значения в соответствии с разработанной нами системой вознаграждений.)
step () используется для воспроизведения действия, выбранного агентом (представленного как действие), вместе с ответом оппонента. Он возвращает:
получившаяся игровая доска (в виде массива numpy),
вознаграждение агента (только за последний ход: +1, -10, -1 или 1/42) и
независимо от того, закончилась ли игра (если игра закончилась, done = True; в противном случае done = False).
Чтобы узнать больше о том, как определять среды, ознакомьтесь с документацией здесь.

```python
from kaggle_environments import make, evaluate
from gym import spaces

class ConnectFourGym:
    def __init__(self, agent2="random"):
        ks_env = make("connectx", debug=True)
        self.env = ks_env.train([None, agent2])
        self.rows = ks_env.configuration.rows
        self.columns = ks_env.configuration.columns
        # Learn about spaces here: http://gym.openai.com/docs/#spaces
        self.action_space = spaces.Discrete(self.columns)
        self.observation_space = spaces.Box(low=0, high=2, 
                                            shape=(self.rows,self.columns,1), dtype=np.int)
        # Tuple corresponding to the min and max possible rewards
        self.reward_range = (-10, 1)
        # StableBaselines throws error if these are not defined
        self.spec = None
        self.metadata = None
    def reset(self):
        self.obs = self.env.reset()
        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)
    def change_reward(self, old_reward, done):
        if old_reward == 1: # The agent won the game
            return 1
        elif done: # The opponent won the game
            return -1
        else: # Reward 1/42
            return 1/(self.rows*self.columns)
    def step(self, action):
        # Check if agent's move is valid
        is_valid = (self.obs['board'][int(action)] == 0)
        if is_valid: # Play the move
            self.obs, old_reward, done, _ = self.env.step(int(action))
            reward = self.change_reward(old_reward, done)
        else: # End the game and penalize agent
            reward, done, _ = -10, True, {}
        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _
```

В этой записной книжке мы научим агента побеждать случайного агента. Мы указываем этого противника в аргументе agent2 ниже.

```
# Создать среду ConnectFour
env = ConnectFourGym (agent2 = "random")
```
Stable Baselines требует, чтобы мы работали с "векторизованными" средами. Для этого мы можем использовать класс DummyVecEnv.

Класс Monitor позволяет нам наблюдать, как производительность агента постепенно улучшается по мере того, как он играет во все больше и больше игр.
```commandline
!apt-get update
!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev
!pip install "stable-baselines[mpi]==2.9.0"
```

```python
import os
from stable_baselines.bench import Monitor 
from stable_baselines.common.vec_env import DummyVecEnv

# Create directory for logging training information
log_dir = "ppo/"
os.makedirs(log_dir, exist_ok=True)

# Logging progress
monitor_env = Monitor(env, log_dir, allow_early_resets=True)

# Create a vectorized environment
vec_env = DummyVecEnv([lambda: monitor_env])
```
Следующим шагом является определение архитектуры нейронной сети. В этом случае мы используем сверточную нейронную сеть. Чтобы узнать больше о том, как указать архитектуры с помощью Stable Baselines, ознакомьтесь с документацией здесь.

Обратите внимание, что это нейронная сеть, которая выводит вероятности выбора каждого столбца. Поскольку мы 
используем алгоритм PPO (PPO1 в ячейке кода ниже), наша сеть также будет выводить некоторую дополнительную информацию (называемую «значением» ввода). Это выходит за рамки этого курса, но вы можете узнать больше, прочитав о «сетях актер-критик». 
```python
from stable_baselines import PPO1 
from stable_baselines.a2c.utils import conv, linear, conv_to_fc
from stable_baselines.common.policies import CnnPolicy

# Neural network for predicting action values
def modified_cnn(scaled_images, **kwargs):
    activ = tf.nn.relu
    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, 
                         init_scale=np.sqrt(2), **kwargs))
    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, 
                         init_scale=np.sqrt(2), **kwargs))
    layer_2 = conv_to_fc(layer_2)
    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  

class CustomCnnPolicy(CnnPolicy):
    def __init__(self, *args, **kwargs):
        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)
        
# Initialize agent
model = PPO1(CustomCnnPolicy, vec_env, verbose=0)
```
В ячейке кода выше веса нейронной сети изначально установлены на случайные значения.

В следующей ячейке кода мы «обучаем агента», что является еще одним способом сказать, что мы находим веса нейронной сети, которые могут привести к тому, что агент выберет правильные ходы.

Мы строим скользящее среднее совокупного вознаграждения, полученного агентом во время обучения. Как видно из 
возрастающей функции, агент постепенно научился работать лучше, играя в игру. 

```python
# Train agent
model.learn(total_timesteps=100000)

# Plot cumulative reward
with open(os.path.join(log_dir, "monitor.csv"), 'rt') as fh:    
    firstline = fh.readline()
    assert firstline[0] == '#'
    df = pd.read_csv(fh, index_col=None)['r']
df.rolling(window=1000).mean().plot()
plt.show()
```
Наконец, мы указываем обученного агента в формате, необходимом для конкурса.

```python
def agent1(obs, config):
    # Use the best model to select a column
    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))
    # Check if selected column is valid
    is_valid = (obs['board'][int(col)] == 0)
    # If not valid, select random move. 
    if is_valid:
        return int(col)
    else:
        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])
```
В следующей ячейке кода мы видим результат одного игрового раунда против случайного агента.
```python
# Create the game environment
env = make("connectx")

# Two random agents play one game round
env.run([agent1, "random"])

# Show the game
env.render(mode="ipython")
```
И мы вычисляем, как он в среднем работает против случайного агента.
```python
def get_win_percentages(agent1, agent2, n_rounds=100):
    # Use default Connect Four setup
    config = {'rows': 6, 'columns': 7, 'inarow': 4}
    # Agent 1 goes first (roughly) half the time          
    outcomes = evaluate("connectx", [agent1, agent2], config, [], n_rounds//2)
    # Agent 2 goes first (roughly) half the time      
    outcomes += [[b,a] for [a,b] in evaluate("connectx", [agent2, agent1], config, [], n_rounds-n_rounds//2)]
    print("Agent 1 Win Percentage:", np.round(outcomes.count([1,-1])/len(outcomes), 2))
    print("Agent 2 Win Percentage:", np.round(outcomes.count([-1,1])/len(outcomes), 2))
    print("Number of Invalid Plays by Agent 1:", outcomes.count([None, 0]))
    print("Number of Invalid Plays by Agent 2:", outcomes.count([0, None]))
```
```python
get_win_percentages(agent1=agent1, agent2="random")
```
```commandline
Agent 1 Win Percentage: 0.83
Agent 2 Win Percentage: 0.17
Number of Invalid Plays by Agent 1: 0
Number of Invalid Plays by Agent 2: 0
```
Важно отметить, что созданный нами агент был обучен только для того, чтобы побеждать случайного агента, потому что весь его игровой процесс был связан со случайным агентом в качестве оппонента.

Если мы хотим создать агента, который надежно работает лучше, чем многие другие агенты, мы должны показать нашего агента этим другим агентам во время обучения. Чтобы узнать больше о том, как это сделать, вы можете прочитать о самостоятельной игре.

Учить больше
Это было очень быстрое и подробное введение в обучение с подкреплением. Если вы хотите более подробно изучить эту тему, рекомендуем ознакомиться со следующими (бесплатными!)























