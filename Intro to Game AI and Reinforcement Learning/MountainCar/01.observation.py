"""
Python 3.9 программа на Python по изучению обучения с подкреплением - Reinforcement Learning
Название файла 01.observation.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2021-12-19

наблюдаем за средой выполнения
"""
import gym  # библиотека OpenAI с простыми играми
import numpy as np  # работа с массивами
env = gym.make("MountainCar-v0")  # обращаемся к виртуальной среде (инициализировать среду)
env.reset()  # В случае с этим тренажерным залом наблюдения возвращаются из сбросов и шагов.

done = False  # переменная характеризующая завершение программы
while not done:
    action = 2  # always go right! 0 означает толчок влево, 1 - оставаться на месте, а 2 - толкать вправо.
    new_state, reward, done, _ = env.step(action)  # считаем текущие характиристики программы (машины)
    print(reward, new_state)  # выведем переменные на экран

# new_state - характеризует положение машины (по оси х и скорость)
# reward -  вознаграждение (-1 всегда кроме достижения желтого флага = 0)
# done - переменная характеризующая завершение программы

# На каждом шаге мы получаем новое состояние, награду, независимо от того, завершена ли среда (либо мы ее превзошли,
# либо исчерпали наш предел в 200 шагов), а затем возвращается окончательная «дополнительная информация»,
# но в этой среде, этот последний элемент возврата не используется. Тренажерный зал добавляет это туда,
# чтобы мы могли использовать одни и те же программы обучения с подкреплением в различных средах без необходимости
# фактически изменять какой-либо код.

# Итак, выше мы видим, что машина двигалась влево, например, потому что скорость отрицательная.
# Имея общее положение и скорость, мы могли бы * определенно * придумать какой-то алгоритм, который мог бы вычислить,
# дойдем ли мы до флага или нет, или вместо этого мы должны снова развернуться, чтобы набрать больше импульса,
# поэтому мы надеюсь, что Q Learning может сделать то же самое. Эти 2 ценности и есть наше «пространство для
# наблюдения». Это пространство может быть любого размера, но чем больше оно становится, тем больше становится
# Q-таблица!
#
# Что такое Q-таблица !?
# Q-Learning работает так, что существует значение «Q» для каждого действия, возможного для каждого состояния. Это
# создает таблицу. Чтобы выяснить все возможные состояния, мы можем либо запросить среду (если она будет достаточно
# любезна, чтобы сообщить нам) ... или нам просто нужно какое-то время погрузиться в среду, чтобы выяснить это.
#
# В нашем случае мы можем запросить среду, чтобы узнать возможные диапазоны для каждого из этих значений состояния:
print('максимальные значения положения и скорости = ', env.observation_space.high)
print('минимальные значения положения и скорости = ',  env.observation_space.low)

# Для значения индекса 0 мы видим максимальное значение 0,6, минимальное значение -1,2, а затем для значения индекса
# 1 максимальное значение 0,07 и минимальное значение -0,07. Хорошо, это диапазоны, но из одного из вышеперечисленных
# состояний наблюдения, которое мы выводим:, [-0.27508804 -0.00268013]мы видим, что эти числа могут стать довольно
# детализированными. Можете ли вы представить себе размер Q-таблицы, если бы у нас было значение для каждой
# комбинации этих диапазонов до 8 знаков после запятой? Это было бы здорово! И, что более важно, бесполезно. Нам не
# нужна такая детализация. Итак, вместо этого мы хотим преобразовать эти непрерывные значения в дискретные. По сути,
# мы хотим объединить / сгруппировать диапазоны во что-то более управляемое.
#
# Мы будем использовать 20 групп / сегментов для каждого диапазона. Это переменная, которую вы можете изменить позже.
DISCRETE_OS_SIZE = [20, 20]
discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE

print('шаг дискритизации положения и скорости = ',discrete_os_win_size)

# Итак, это фигура 20x20x3, которая инициализировала для нас случайные значения Q. Бит 20 x 20 - это каждая
# комбинация сегментов сегмента всех возможных состояний. Бит x3 предназначен для всех возможных действий,
# которые мы можем предпринять.
#
# Как вы, вероятно, уже заметили ... даже в этой простой среде есть довольно большая таблица. У нас есть ценность для
# каждого возможного состояния!
q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))
print('\n произвольная таблица состояний \n', q_table)

