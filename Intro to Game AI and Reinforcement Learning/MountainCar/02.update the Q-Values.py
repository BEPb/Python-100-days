"""
Python 3.9 программа на Python по изучению обучения с подкреплением - Reinforcement Learning
Название файла 01.observation.py

Version: 0.1
Author: Andrej Marinchenko
Date: 2021-12-19

Добро пожаловать во вторую часть серии руководств по обучению с подкреплением, особенно с Q-Learning. Мы создали
нашу Q-таблицу, которая содержит все наши возможные дискретные состояния. Затем нам нужен способ обновить
Q-значения (значение для возможного действия для каждого уникального состояния), что привело нас к:

new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)

Это DISCOUNT показатель того, насколько мы хотим заботиться о БУДУЩЕМ вознаграждении, а не о немедленном
вознаграждении. Как правило, это значение будет довольно высоким и находится в диапазоне от 0 до 1. Мы хотим,
чтобы оно было высоким, потому что цель Q Learning действительно состоит в том, чтобы изучить цепочку событий,
которая заканчивается положительным результатом, поэтому вполне естественно, что мы придаем большее значение в
долгосрочной перспективе, а не в краткосрочной.

max_future_q Хватают после того как мы проводили наши действия уже, а затем мы обновляем наши предыдущие значения,
основанные частично на лучшем значении Q следующего СТЭПА. Со временем, как только мы достигли цели один раз,
это значение «награды» медленно возвращается назад, шаг за шагом, за эпизод. Супер базовая концепция, но довольно
изящная, как она работает!

Итак, теперь мы знаем буквально все, что нам нужно знать, чтобы это работало. На самом деле это совсем не
«алгоритмический» код, нам просто нужно написать окружающую логику.
"""
import gym  # библиотека OpenAI с простыми играми
import numpy as np  # работа с массивами
env = gym.make("MountainCar-v0")  # обращаемся к виртуальной среде (инициализировать среду)
env.reset()  # В случае с этим тренажерным залом наблюдения возвращаются из сбросов и шагов.

DISCRETE_OS_SIZE = [20, 20]  # задаем таблицу дискритизации возможных состояний машины
discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE  # шаг дискритизации

q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))  # заполняем таблицу
# случайными значениями

done = False  # переменная характеризующая завершение программы

while not done:
    action = 2  # always go right! 0 означает толчок влево, 1 - оставаться на месте, а 2 - толкать вправо.
    new_state, reward, done, _ = env.step(action)  # считаем текущие характиристики программы (машины)
    print(reward, new_state)  # выведем переменные на экран
    env.render()
    #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)


