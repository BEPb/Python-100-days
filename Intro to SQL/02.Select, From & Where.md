# Select, From & Where
### Введение
Теперь, когда вы знаете, как получить доступ к набору данных и изучить его, вы готовы написать свой первый 
SQL-запрос! Как вы скоро увидите, SQL-запросы помогут вам отсортировать огромный набор данных, чтобы получить только 
ту информацию, которая вам нужна.

Мы начнем с использования ключевых слов SELECT, FROM и WHERE для получения данных из определенных столбцов на основе 
заданных вами условий.

Для ясности мы будем работать с небольшим воображаемым набором данных pet_records, который содержит только одну 
таблицу с именем pets. 



### ВЫБРАТЬ ИЗ
Самый простой SQL-запрос выбирает один столбец из одной таблицы. Сделать это,

- укажите нужный столбец после слова SELECT, а затем
- укажите таблицу после слова FROM.
Например, чтобы выбрать столбец Name (из таблицы pets в базе данных pet_records в проекте bigquery-public-data), наш 
  запрос будет выглядеть следующим образом: 

Обратите внимание, что при написании SQL-запроса аргумент, который мы передаем в FROM, не заключен в одинарные или 
двойные кавычки (' или "). Он заключен в обратные кавычки (`). 

### ГДЕ ...
Наборы данных BigQuery большие, поэтому обычно требуется возвращать только те строки, которые соответствуют 
определенным условиям. Вы можете сделать это, используя предложение WHERE. 

Приведенный ниже запрос возвращает записи из столбца «Имя», которые находятся в строках, где в столбце «Животное» 
есть текст «Кошка». 



### Пример: каковы все города США в наборе данных OpenAQ?
Теперь, когда вы разобрались с основами, давайте рассмотрим пример с реальным набором данных. Мы будем использовать 
набор данных OpenAQ о качестве воздуха. 

Во-первых, мы настроим все необходимое для выполнения запросов и быстро посмотрим, какие таблицы есть в нашей базе 
данных. 
```python
from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "openaq" dataset
dataset_ref = client.dataset("openaq", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "openaq" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there's only one!)
for table in tables:  
    print(table.table_id)
```
Набор данных содержит только одну таблицу с именем global_air_quality. Мы получим таблицу и взглянем на первые 
несколько строк, чтобы увидеть, какие данные она содержит. 
```python
# Construct a reference to the "global_air_quality" table
table_ref = dataset_ref.table("global_air_quality")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "global_air_quality" table
client.list_rows(table, max_results=5).to_dataframe()
```
Все выглядит хорошо! Итак, составим запрос. Допустим, мы хотим выбрать все значения из столбца города, которые 
находятся в строках, где столбец страны имеет значение «США» (для «Соединенных Штатов»). 
```python
# Query to select all the items from the "city" column where the "country" column is 'US'
query = """
        SELECT city
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """
```
Потратьте время сейчас, чтобы убедиться, что этот запрос соответствует тому, что вы узнали выше.

### Отправка запроса к набору данных
Мы готовы использовать этот запрос для получения информации из набора данных OpenAQ. Как и в предыдущем руководстве, 
первым шагом является создание объекта Client. 
```python
# Create a "Client" object
client = bigquery.Client()
```
Начнем с настройки запроса с помощью метода query(). Мы запускаем метод с параметрами по умолчанию, но этот метод 
также позволяет нам указать более сложные настройки, о которых вы можете прочитать в документации. Мы вернемся к 
этому позже.  

```python
# Set up the query
query_job = client.query(query)
```
Затем мы запускаем запрос и конвертируем результаты в pandas DataFrame.
```python
# API request - run the query, and return a pandas DataFrame
us_cities = query_job.to_dataframe()
```
Теперь у нас есть DataFrame pandas с именем us_cities, который мы можем использовать как любой другой DataFrame.

```python
# What five cities have the most measurements?
us_cities.city.value_counts().head()
```
### Больше запросов
Если вам нужно несколько столбцов, вы можете выбрать их с запятой между именами:
```python
query = """
        SELECT city, country
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """
```
Вы можете выбрать все столбцы с помощью * следующим образом:
```python
query = """
        SELECT *
        FROM `bigquery-public-data.openaq.global_air_quality`
        WHERE country = 'US'
        """
```
### Вопросы и ответы: примечания по форматированию
Форматирование SQL-запроса может показаться незнакомым. Если у вас есть какие-либо вопросы, вы можете задать их в 
разделе комментариев внизу этой страницы. Вот ответы на два распространенных вопроса: 

- Вопрос: Что не так с тройными кавычками (""")?
- Ответ: они сообщают Python, что все внутри них представляет собой одну строку, даже если в ней есть разрывы строк. 
  Разрывы строк необязательны, но они облегчают чтение вашего запроса.

- Вопрос. Нужно ли использовать заглавные буквы в SELECT и FROM?
- Ответ: Нет, SQL не заботится о капитализации. Однако принято писать команды SQL с заглавной буквы, и это делает 
  ваши запросы более удобными для чтения.

### Работа с большими наборами данных
Наборы данных BigQuery могут быть огромными. Мы позволяем вам делать много вычислений бесплатно, но у всех есть 
ограничения. 

Каждый пользователь Kaggle может бесплатно сканировать 5 ТБ каждые 30 дней. Как только вы достигнете этого предела, 
вам придется дождаться его сброса. 

Самый большой набор данных в настоящее время на Kaggle составляет 3 ТБ, поэтому вы можете пройти свой 30-дневный 
лимит за пару запросов, если не будете осторожны. 

Но не волнуйтесь: мы научим вас, как избежать одновременного сканирования слишком большого количества данных, чтобы 
не превысить лимит. 

Для начала вы можете оценить размер любого запроса перед его выполнением. Вот пример с использованием (очень 
большого!) набора данных Hacker News. Чтобы увидеть, сколько данных будет сканировать запрос, мы создаем объект 
QueryJobConfig и устанавливаем для параметра dry_run значение True.  

```python
# Query to get the score column from every row where the type column has value "job"
query = """
        SELECT score, title
        FROM `bigquery-public-data.hacker_news.full`
        WHERE type = "job" 
        """

# Create a QueryJobConfig object to estimate size of query without running it
dry_run_config = bigquery.QueryJobConfig(dry_run=True)

# API request - dry run query to estimate costs
dry_run_query_job = client.query(query, job_config=dry_run_config)

print("This query will process {} bytes.".format(dry_run_query_job.total_bytes_processed))
```
Вы также можете указать параметр при выполнении запроса, чтобы ограничить объем данных, которые вы готовы 
сканировать. Вот пример с низким лимитом. 
```python
# Only run the query if it's less than 1 MB
ONE_MB = 1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)

# Set up the query (will only run if it's less than 1 MB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
safe_query_job.to_dataframe()
```
В этом случае запрос был отменен, так как превышен лимит в 1 МБ. Однако мы можем увеличить лимит для успешного 
выполнения запроса! 
```python
# Only run the query if it's less than 1 GB
ONE_GB = 1000*1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)

# Set up the query (will only run if it's less than 1 GB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
job_post_scores = safe_query_job.to_dataframe()

# Print average score for job posts
job_post_scores.score.mean()
```







