### Deep Neural Networks

### Введение
В этом уроке мы увидим, как мы можем построить нейронные сети, способные изучать сложные виды отношений, которыми 
славятся глубокие нейронные сети. 

Ключевой идеей здесь является модульность, построение сложной сети из более простых функциональных единиц. Мы видели,
как линейная единица вычисляет линейную функцию - теперь мы увидим, как комбинировать и изменять эти отдельные 
единицы для моделирования более сложных отношений.  

### Слои
Нейронные сети обычно организуют свои нейроны в слои. Когда мы собираем вместе линейные единицы, имеющие общий набор 
входных данных, мы получаем плотный слой. 

Вы можете думать о каждом слое нейронной сети как о выполнении некоторого относительно простого преобразования. 
Благодаря глубокому стеку слоев нейронная сеть может преобразовывать свои входные данные все более и более сложными 
способами. В хорошо обученной нейронной сети каждый слой представляет собой преобразование, которое немного 
приближает нас к решению.   

### Множество видов слоев
«Слой» в Керасе - вещь очень общего вида. Слой может быть, по сути, любым видом преобразования данных. Многие слои, 
такие как сверточные и повторяющиеся слои, преобразуют данные с помощью нейронов и различаются в первую очередь 
образцом соединений, которые они формируют. Другие же используются для разработки функций или просто для арифметики. 
Вам предстоит открыть для себя целый мир слоев - проверьте их!   

### Функция активации
Однако оказывается, что два плотных слоя, между которыми ничего нет, ничем не лучше, чем один плотный слой сам по 
себе. Сами по себе плотные слои никогда не могут вывести нас из мира линий и плоскостей. Нам нужно что-то нелинейное.
Нам нужны функции активации.  


Функция активации - это просто некоторая функция, которую мы применяем к каждому выходу слоя (его активации). 
Наиболее распространенной является функция выпрямителя max (0, x). 

График функции выпрямителя. Линия y = x, когда x> 0, и y = 0, когда x <0, образует "шарнирную" форму, подобную "_ /".
График функции выпрямителя представляет собой линию с отрицательной частью, «выпрямленной» до нуля. Применение 
функции к выходам нейрона приведет к изгибу данных, уводя нас от простых линий. 

Когда мы присоединяем выпрямитель к линейному блоку, мы получаем выпрямленный линейный блок или ReLU. (По этой 
причине функцию выпрямителя принято называть «функцией ReLU».) Применение активации ReLU к линейному блоку означает, 
что выход становится max (0, w * x + b), что мы могли бы нарисовать на диаграмме вроде :  

Схема одиночного ReLU. Как линейная единица, но вместо символа «+» теперь у нас есть петля «_ /».
Выпрямленный линейный блок.

### Укладка плотных слоев
Теперь, когда у нас есть некоторая нелинейность, давайте посмотрим, как мы можем складывать слои для получения 
сложных преобразований данных. 

Входной слой, два скрытых слоя и последний линейный слой.
Стек плотных слоев образует «полностью связанную» сеть.
Слои перед выходным слоем иногда называют скрытыми, поскольку мы никогда не видим их выходы напрямую.

Теперь обратите внимание, что последний (выходной) слой представляет собой линейную единицу (то есть без функции 
активации). Это делает эту сеть подходящей для задачи регрессии, когда мы пытаемся предсказать какое-то произвольное 
числовое значение. Для других задач (например, классификации) может потребоваться функция активации на выходе.  

### Построение последовательных моделей
Последовательная модель, которую мы использовали, будет соединять вместе список слоев в порядке от первого до 
последнего: первый слой получает входные данные, последний слой производит выходные данные. Это создает модель на 
рисунке  
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # the hidden ReLU layers
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # the linear output layer 
    layers.Dense(units=1),
])
```

Обязательно передайте все слои вместе в списке, например [слой, слой, слой, ...], а не в качестве отдельных 
аргументов. Чтобы добавить функцию активации к слою, просто укажите ее имя в аргументе активации. 