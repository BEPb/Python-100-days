### Stochastic Gradient Descent
### Введение
В первых двух уроках мы узнали, как строить полносвязные сети из стопок плотных слоев. При первом создании все веса 
сети устанавливаются случайным образом - сеть еще ничего не «знает». В этом уроке мы увидим, как обучить нейронную 
сеть; мы собираемся посмотреть, как обучаются нейронные сети.  
 
Как и все задачи машинного обучения, мы начинаем с набора обучающих данных. Каждый пример обучающих данных состоит 
из некоторых функций (входных данных) вместе с ожидаемой целью (выходными данными). Обучение сети означает 
корректировку ее весов таким образом, чтобы она могла преобразовать объекты в целевые. В наборе данных 80 зерновых, 
например, нам нужна сеть, которая может брать «сахар», «клетчатку» и «белок» в каждом зерне и давать прогноз для 
«калорий» этого злака. Если мы можем успешно обучить сеть этому, ее веса должны каким-то образом отражать 
взаимосвязь между этими функциями и этой целью, как это выражено в данных обучения.     

Помимо обучающих данных нам понадобятся еще две вещи:

- «Функция потерь», которая измеряет, насколько хороши прогнозы сети.
- «Оптимизатор», который может сказать сети, как изменить ее веса.

### Функция потерь
Мы видели, как проектировать архитектуру сети, но не видели, как сообщить сети, какую проблему следует решить. Это 
работа функции потерь. 

Функция потерь измеряет несоответствие между истинным значением цели и значением, предсказываемым моделью.

Для разных задач требуются разные функции потерь. Мы изучали проблемы регрессии, где задача состоит в том, чтобы 
предсказать некоторое числовое значение - калорийность в 80 злаках, оценка качества красного вина. Другие задачи 
регрессии могут заключаться в прогнозировании цены дома или топливной экономичности автомобиля.  

Обычной функцией потерь для задач регрессии является средняя абсолютная ошибка или MAE. Для каждого прогноза y_pred 
MAE измеряет отклонение от истинного целевого значения y_true по абсолютной разнице abs (y_true - y_pred). 

Общие потери MAE в наборе данных - это среднее значение всех этих абсолютных различий.

График, показывающий планки погрешностей от точек данных до подобранной линии.
Средняя абсолютная ошибка - это средняя длина между подобранной кривой и точками данных.
Помимо MAE, другие функции потерь, которые вы можете увидеть для проблем регрессии, - это среднеквадратичная ошибка 
(MSE) или потеря Хубера (обе доступны в Keras). 

Во время обучения модель будет использовать функцию потерь в качестве ориентира для поиска правильных значений ее 
весов (чем меньше потери, тем лучше). Другими словами, функция потерь сообщает сети ее цель. 

### Оптимизатор - стохастический градиентный спуск
Мы описали проблему, которую должна решить сеть, но теперь нам нужно сказать, как ее решить. Это работа оптимизатора.
Оптимизатор - это алгоритм, который регулирует веса, чтобы минимизировать потери. 

Практически все алгоритмы оптимизации, используемые в глубоком обучении, принадлежат к семейству, называемому 
стохастическим градиентным спуском. Это итерационные алгоритмы, которые пошагово обучают сеть. Один этап обучения 
проходит так:  

- Выберите некоторые обучающие данные и прогоните их по сети, чтобы сделать прогнозы.
- Измерьте потерю между предсказаниями и истинными значениями.
- Наконец, отрегулируйте веса так, чтобы потери были меньше.
- Затем просто делайте это снова и снова, пока потеря не станет настолько маленькой, насколько вам нравится (или пока 
она больше не уменьшится). 

Оснащение линии партия за партией. Потери уменьшаются, и веса приближаются к своим истинным значениям.

### Обучение нейронной сети со стохастическим градиентным спуском (SGD).
Выборка обучающих данных на каждой итерации называется минипакетом (или часто просто «пакетом»), а полный цикл 
обучающих данных называется эпохой. Количество эпох, в течение которых вы тренируетесь, - это то, сколько раз сеть 
увидит каждый обучающий пример.

![](./imgs/00.gif)

Анимация показывает, как линейная модель из Урока 1 обучается с помощью SGD. Бледно-красные точки обозначают весь 
обучающий набор, а сплошные красные точки - мини-пакеты. Каждый раз, когда SGD видит новую мини-серию, она сдвигает 
веса (w наклон и b точка пересечения по оси Y) в сторону их правильных значений для этой партии. Партия за партией, 
линия в конечном итоге сходится до наилучшего соответствия. Вы можете видеть, что потери становятся меньше по мере 
приближения весов к их истинным значениям.    

### Скорость обучения и размер пакета
Обратите внимание, что линия только немного смещается в направлении каждой партии (вместо того, чтобы двигаться 
полностью). Размер этих смен определяется скоростью обучения. Меньшая скорость обучения означает, что сети 
необходимо увидеть больше мини-пакетов, прежде чем ее веса приблизятся к своим лучшим значениям.  

Скорость обучения и размер мини-пакетов - два параметра, которые имеют наибольшее влияние на то, как проходит 
обучение SGD. Их взаимодействие часто бывает незаметным, и правильный выбор этих параметров не всегда очевиден. (Мы 
исследуем эти эффекты в упражнении.)  

К счастью, для большинства работ нет необходимости проводить обширный поиск гиперпараметров, чтобы получить 
удовлетворительный результат. 

```python
model.compile(
    optimizer="adam",
    loss="mae",
)
```

Обратите внимание, что мы можем указать потерю и оптимизатор только с помощью строки. Вы также можете получить к ним 
доступ напрямую через Keras API - например, если вы хотите настроить параметры - но для нас значения по умолчанию 
будут работать нормально.  

### Что в имени?
Градиент - это вектор, который сообщает нам, в каком направлении должны двигаться веса. Точнее, он говорит нам, как 
изменить веса, чтобы убытки изменились быстрее. Мы называем наш процесс градиентным спуском, потому что он 
использует градиент для спуска кривой потерь к минимуму. Стохастик означает «определяется случайно». Наше обучение 
является стохастическим, потому что мини-пакеты представляют собой случайные выборки из набора данных. И поэтому он 
называется SGD!    

Пример - качество красного вина
Теперь мы знаем все, что нужно, чтобы приступить к обучению моделей глубокого обучения. Итак, давайте посмотрим на 
это в действии! Мы будем использовать набор данных качества красного вина. 

Этот набор данных состоит из физико-химических измерений примерно 1600 португальских красных вин. Также включен 
рейтинг качества каждого вина по результатам слепых дегустационных тестов. Насколько хорошо мы можем предсказать 
воспринимаемое качество вина по этим измерениям?  

Мы поместили всю подготовку данных в следующую скрытую ячейку. Это не обязательно, поэтому не стесняйтесь его 
пропустить. Однако сейчас вы могли бы отметить одну вещь: мы изменили масштаб каждой функции, чтобы она находилась в 
интервале [0,1]. Как мы обсудим более подробно в Уроке 5, нейронные сети, как правило, работают лучше всего, когда 
их входные данные имеют общий масштаб.   

```python
import pandas as pd
from IPython.display import display

red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')

# Create training and validation splits
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)
display(df_train.head(4))

# Scale to [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)

# Split features and target
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```
Сколько входов должна иметь эта сеть? Мы можем обнаружить это, посмотрев на количество столбцов в матрице данных. 
Убедитесь, что здесь не указано целевое значение («качество») - только входные характеристики. 


```python
print(X_train.shape)
```
Одиннадцать столбцов означают одиннадцать входов.

Мы выбрали трехуровневую сеть с более чем 1500 нейронами. Эта сеть должна быть способна изучать довольно сложные 
отношения в данных. 
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
```
Выбор архитектуры вашей модели должен быть частью процесса. Начните с простого и используйте потерю проверки в 
качестве ориентира. Вы узнаете больше о разработке моделей в упражнениях. 

После определения модели мы компилируем оптимизатор и функцию потерь.

```python
model.compile(
    optimizer='adam',
    loss='mae',
)
```
Теперь мы готовы приступить к обучению! Мы сказали Керасу подавать оптимизатору 256 строк обучающих данных за раз 
(batch_size) и делать это 10 раз на всем протяжении набора данных (эпохи). 

```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```
Вы можете видеть, что Керас будет держать вас в курсе потери, пока модель тренируется.

Часто лучший способ увидеть потерю - это нанести ее на график. Фактически метод подгонки ведет учет потерь, 
возникших во время обучения в объекте History. Мы преобразуем данные в фреймворк Pandas, что упростит построение 
графика.  

```python
import pandas as pd

# convert the training history to a dataframe
history_df = pd.DataFrame(history.history)
# use Pandas native plot method
history_df['loss'].plot();
```
Обратите внимание, как с течением времени потери выравниваются. Когда кривая потерь становится такой горизонтальной, 
это означает, что модель усвоила все, что могла, и не было бы причин для продолжения дополнительных эпох. 



