### Stochastic Gradient Descent
### Введение
В первых двух уроках мы узнали, как строить полносвязные сети из стопок плотных слоев. При первом создании все веса 
сети устанавливаются случайным образом - сеть еще ничего не «знает». В этом уроке мы увидим, как обучить нейронную 
сеть; мы собираемся посмотреть, как обучаются нейронные сети.  
 
Как и все задачи машинного обучения, мы начинаем с набора обучающих данных. Каждый пример обучающих данных состоит 
из некоторых функций (входных данных) вместе с ожидаемой целью (выходными данными). Обучение сети означает 
корректировку ее весов таким образом, чтобы она могла преобразовать объекты в целевые. В наборе данных 80 зерновых, 
например, нам нужна сеть, которая может брать «сахар», «клетчатку» и «белок» в каждом зерне и давать прогноз для 
«калорий» этого злака. Если мы можем успешно обучить сеть этому, ее веса должны каким-то образом отражать 
взаимосвязь между этими функциями и этой целью, как это выражено в данных обучения.     

Помимо обучающих данных нам понадобятся еще две вещи:

- «Функция потерь», которая измеряет, насколько хороши прогнозы сети.
- «Оптимизатор», который может сказать сети, как изменить ее веса.

### Функция потерь
Мы видели, как проектировать архитектуру сети, но не видели, как сообщить сети, какую проблему следует решить. Это 
работа функции потерь. 

Функция потерь измеряет несоответствие между истинным значением цели и значением, предсказываемым моделью.

Для разных задач требуются разные функции потерь. Мы изучали проблемы регрессии, где задача состоит в том, чтобы 
предсказать некоторое числовое значение - калорийность в 80 злаках, оценка качества красного вина. Другие задачи 
регрессии могут заключаться в прогнозировании цены дома или топливной экономичности автомобиля.  

Обычной функцией потерь для задач регрессии является средняя абсолютная ошибка или MAE. Для каждого прогноза y_pred 
MAE измеряет отклонение от истинного целевого значения y_true по абсолютной разнице abs (y_true - y_pred). 

Общие потери MAE в наборе данных - это среднее значение всех этих абсолютных различий.

График, показывающий планки погрешностей от точек данных до подобранной линии.
Средняя абсолютная ошибка - это средняя длина между подобранной кривой и точками данных.
Помимо MAE, другие функции потерь, которые вы можете увидеть для проблем регрессии, - это среднеквадратичная ошибка 
(MSE) или потеря Хубера (обе доступны в Keras). 

Во время обучения модель будет использовать функцию потерь в качестве ориентира для поиска правильных значений ее 
весов (чем меньше потери, тем лучше). Другими словами, функция потерь сообщает сети ее цель. 

### Оптимизатор - стохастический градиентный спуск
Мы описали проблему, которую должна решить сеть, но теперь нам нужно сказать, как ее решить. Это работа оптимизатора.
Оптимизатор - это алгоритм, который регулирует веса, чтобы минимизировать потери. 

Практически все алгоритмы оптимизации, используемые в глубоком обучении, принадлежат к семейству, называемому 
стохастическим градиентным спуском. Это итерационные алгоритмы, которые пошагово обучают сеть. Один этап обучения 
проходит так:  

- Выберите некоторые обучающие данные и прогоните их по сети, чтобы сделать прогнозы.
- Измерьте потерю между предсказаниями и истинными значениями.
- Наконец, отрегулируйте веса так, чтобы потери были меньше.
- Затем просто делайте это снова и снова, пока потеря не станет настолько маленькой, насколько вам нравится (или пока 
она больше не уменьшится). 

Оснащение линии партия за партией. Потери уменьшаются, и веса приближаются к своим истинным значениям.

### Обучение нейронной сети со стохастическим градиентным спуском (SGD).
Выборка обучающих данных на каждой итерации называется минипакетом (или часто просто «пакетом»), а полный цикл 
обучающих данных называется эпохой. Количество эпох, в течение которых вы тренируетесь, - это то, сколько раз сеть 
увидит каждый обучающий пример.

![](Intro_to_Deep_Learning/imgs/00.gif)

Анимация показывает, как линейная модель из Урока 1 обучается с помощью SGD. Бледно-красные точки обозначают весь 
обучающий набор, а сплошные красные точки - мини-пакеты. Каждый раз, когда SGD видит новую мини-серию, она сдвигает 
веса (w наклон и b точка пересечения по оси Y) в сторону их правильных значений для этой партии. Партия за партией, 
линия в конечном итоге сходится до наилучшего соответствия. Вы можете видеть, что потери становятся меньше по мере 
приближения весов к их истинным значениям.    

### Скорость обучения и размер пакета
Обратите внимание, что линия только немного смещается в направлении каждой партии (вместо того, чтобы двигаться 
полностью). Размер этих смен определяется скоростью обучения. Меньшая скорость обучения означает, что сети 
необходимо увидеть больше мини-пакетов, прежде чем ее веса приблизятся к своим лучшим значениям.  

Скорость обучения и размер мини-пакетов - два параметра, которые имеют наибольшее влияние на то, как проходит 
обучение SGD. Их взаимодействие часто бывает незаметным, и правильный выбор этих параметров не всегда очевиден. (Мы 
исследуем эти эффекты в упражнении.)  

К счастью, для большинства работ нет необходимости проводить обширный поиск гиперпараметров, чтобы получить 
удовлетворительный результат. 

```python
model.compile(
    optimizer="adam",
    loss="mae",
)
```