# Binary Classification
### Введение
До сих пор в этом курсе мы узнали о том, как нейронные сети могут решать проблемы регрессии. Теперь мы собираемся применить нейронные сети к другой распространенной проблеме машинного обучения: классификации. Практически все, что мы узнали до сих пор, все еще применимо. Основное различие заключается в используемой нами функции потерь и в том, какие выходные данные мы хотим, чтобы последний слой производил.

Бинарная классификация
Классификация на один из двух классов - распространенная проблема машинного обучения. Возможно, вы захотите предсказать, совершит ли клиент покупку, была ли транзакция по кредитной карте мошеннической, указывают ли сигналы из дальнего космоса доказательства существования новой планеты или медицинские тесты, свидетельствующие о болезни. Все это проблемы бинарной классификации.

В ваших необработанных данных классы могут быть представлены такими строками, как «Да» и «Нет» или «Собака» и «Кошка». Перед использованием этих данных мы назначим метку класса: один класс будет равен 0, а другой - 1. Назначение числовых меток помещает данные в форму, которую может использовать нейронная сеть.

Точность и кросс-энтропия
Точность - одна из многих метрик, используемых для измерения успеха при решении задачи классификации. Точность - это отношение правильных прогнозов к общему количеству прогнозов: точность = число_правильно / всего. Модель, которая всегда предсказывала правильно, будет иметь показатель точности 1,0. При прочих равных, точность является разумной метрикой для использования всякий раз, когда классы в наборе данных встречаются примерно с одинаковой частотой.

Проблема с точностью (и большинством других показателей классификации) заключается в том, что ее нельзя использовать в качестве функции потерь. SGD нужна функция потерь, которая изменяется плавно, но точность, являющаяся соотношением отсчетов, изменяется «скачкообразно». Итак, мы должны выбрать замену, которая будет действовать как функция потерь. Этим заменителем является функция кросс-энтропии.

Напомним, что функция потерь определяет цель сети во время обучения. При регрессии нашей целью было минимизировать расстояние между ожидаемым и прогнозируемым результатом. Мы выбрали MAE для измерения этого расстояния.

Вместо этого для классификации нам нужно расстояние между вероятностями, и это то, что обеспечивает кросс-энтропия. Кросс-энтропия - это своего рода мера расстояния от одного распределения вероятностей до другого.

Графики точности и кросс-энтропии.
Кросс-энтропия наказывает неправильные предсказания вероятности.
Идея состоит в том, чтобы наша сеть предсказывала правильный класс с вероятностью 1,0. Чем дальше прогнозируемая вероятность от 1,0, тем больше будет потеря кросс-энтропии.

Технические причины, по которым мы используем кросс-энтропию, немного тонки, но главное, что нужно вынести из этого раздела, это просто следующее: использовать кросс-энтропию для потери классификации; другие показатели, которые могут вас заинтересовать (например, точность), будут улучшаться вместе с этим.

Создание вероятностей с помощью сигмовидной функции
Для функций кросс-энтропии и точности требуются вероятности в качестве входных данных, то есть числа от 0 до 1. Чтобы преобразовать действительные выходные данные, производимые плотным слоем, в вероятности, мы добавляем новый вид функции активации - сигмовидную активацию.

Сигмоидальный график представляет собой S-образную форму с горизонтальными асимптотами в 0 слева и 1 справа.
Сигмоидальная функция отображает действительные числа в интервал [0,1].
Чтобы получить окончательный прогноз класса, мы определяем пороговую вероятность. Обычно это будет 0,5, так что округление даст нам правильный класс: ниже 0,5 означает класс с меткой 0, а 0,5 или выше означает класс с меткой 1. Порог 0,5 - это то, что Keras использует по умолчанию с метрикой точности.

Пример - двоичная классификация
А теперь попробуем!

Набор данных ионосферы содержит характеристики, полученные по радиолокационным сигналам, сфокусированным на 
ионосферном слое атмосферы Земли. Задача - определить, показывает ли сигнал наличие какого-либо объекта или просто 
пустой воздух.  
```python
import pandas as pd
from IPython.display import display

ion = pd.read_csv('../input/dl-course-data/ion.csv', index_col=0)
display(ion.head())

df = ion.copy()
df['Class'] = df['Class'].map({'good': 0, 'bad': 1})

df_train = df.sample(frac=0.7, random_state=0)
df_valid = df.drop(df_train.index)

max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)

df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)
df_train.dropna(axis=1, inplace=True) # drop the empty feature in column 2
df_valid.dropna(axis=1, inplace=True)

X_train = df_train.drop('Class', axis=1)
X_valid = df_valid.drop('Class', axis=1)
y_train = df_train['Class']
y_valid = df_valid['Class']
```
Мы определим нашу модель так же, как мы это делали для задач регрессии, за одним исключением. На последнем уровне 
включите активацию «сигмоида», чтобы модель создавала вероятности классов. 
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])
```
Добавьте в модель метрику потерь кросс-энтропии и точности с помощью метода компиляции. Для задач с двумя классами 
обязательно используйте «двоичные» версии. (Проблемы с большим количеством классов будут немного другими.) 
Оптимизатор Adam отлично работает и для классификации, поэтому мы будем его придерживаться.  
```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'],
)
```
Для завершения обучения модели в этой конкретной задаче может потребоваться довольно много времени, поэтому для 
удобства мы включим обратный вызов для ранней остановки. 
```python
early_stopping = keras.callbacks.EarlyStopping(
    patience=10,
    min_delta=0.001,
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=1000,
    callbacks=[early_stopping],
    verbose=0, # hide the output because we have so many epochs
)
```
Мы, как всегда, рассмотрим кривые обучения, а также проверим лучшие значения потерь и точности, которые мы получили 
на проверочном наборе. (Помните, что ранняя остановка восстановит веса до тех, которые получили эти значения.) 

```python
history_df = pd.DataFrame(history.history)
# Start the plot at epoch 5
history_df.loc[5:, ['loss', 'val_loss']].plot()
history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()

print(("Best Validation Loss: {:0.4f}" +\
      "\nBest Validation Accuracy: {:0.4f}")\
      .format(history_df['val_loss'].min(), 
              history_df['val_binary_accuracy'].max()))
```


