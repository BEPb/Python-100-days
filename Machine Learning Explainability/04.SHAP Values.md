# SHAP Values
Введение
Вы видели (и использовали) методы извлечения общих сведений из модели машинного обучения. Но что, если вы хотите понять, как работает модель для индивидуального прогноза?

Значения SHAP (аббревиатура от Shapley Additive exPlanations) разбивают прогноз, чтобы показать влияние каждой функции. Где вы могли бы это использовать?

Модель говорит, что банк не должен ссужать кому-либо деньги, и по закону банк обязан объяснять основания для каждого отказа в ссуде.
Поставщик медицинских услуг хочет определить, какие факторы влияют на риск возникновения у каждого пациента того или иного заболевания, чтобы он мог напрямую устранить эти факторы риска с помощью целенаправленных медицинских вмешательств.
В этом уроке вы будете использовать значения SHAP для объяснения индивидуальных прогнозов. На следующем уроке вы увидите, как их можно объединить в мощные аналитические данные на уровне модели.

Как они работают
Значения SHAP интерпретируют влияние наличия определенного значения для данной функции по сравнению с прогнозом, который мы сделали бы, если бы эта функция приняла некоторое базовое значение.

Пример полезен, и мы продолжим пример футбола/футбола из уроков важности перестановок и графиков частичной зависимости.

В этих руководствах мы предсказывали, будет ли игрок команды выигрывать награду Лучшего игрока матча.

Мы могли бы спросить:

Насколько прогноз был обусловлен тем, что команда забила 3 ​​гола?
Но проще дать конкретный числовой ответ, если мы переформулируем это как:

Насколько прогноз был обусловлен тем фактом, что команда забила 3 ​​гола, а не какое-то базовое количество голов.
Конечно, каждая команда имеет множество особенностей. Итак, если мы ответим на этот вопрос для количества целей, мы можем повторить процесс для всех других функций.

Значения SHAP делают это таким образом, чтобы гарантировать хорошее свойство. В частности, вы разлагаете прогноз с помощью следующего уравнения:

сумма (значения SHAP для всех функций) = pred_for_team - pred_for_baseline_values

То есть значения SHAP всех признаков суммируются, чтобы объяснить, почему мой прогноз отличался от исходного. Это позволяет нам разложить прогноз на такой график:

Имгур

Если вы хотите увеличить этот график, вот ссылка

Как вы это понимаете?

Мы предсказали 0,7, тогда как базовое значение равно 0,4979. Значения функции, вызывающие увеличение прогнозов, выделены розовым цветом, а их визуальный размер показывает величину эффекта функции. Значения признаков, уменьшающие предсказание, выделены синим цветом. Наибольшее влияние оказывает тот факт, что забитый гол равен 2. Хотя значение владения мячом оказывает значимое влияние, уменьшая прогноз.

Если вы вычтете длину синих полос из длины розовых полос, это будет равно расстоянию от базового значения до вывода.

В этом методе есть некоторая сложность, чтобы гарантировать, что базовый уровень плюс сумма отдельных эффектов составляют прогноз (что не так просто, как кажется). Мы не будем вдаваться в подробности здесь, так как это не критично для использования техники. Этот пост в блоге имеет более длинное теоретическое объяснение.

Код для расчета значений SHAP
Мы рассчитываем значения SHAP с помощью замечательной библиотеки Shap.

В этом примере мы будем повторно использовать модель, которую вы уже видели, с данными Soccer.
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')
y = (data['Man of the Match'] == "Yes")  # Convert from string "Yes"/"No" to binary
feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)
```
Мы рассмотрим значения SHAP для одной строки набора данных (мы произвольно выбрали строку 5). Для контекста мы 
рассмотрим необработанные прогнозы, прежде чем рассматривать значения SHAP. 

```python
row_to_show = 5
data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)


my_model.predict_proba(data_for_prediction_array)
```
Вероятность того, что игрок выиграет награду, составляет 70%.

Теперь мы перейдем к коду, чтобы получить значения SHAP для этого единственного прогноза.

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)
```

Приведенный выше объект shap_values представляет собой список с двумя массивами. Первый массив — это значения SHAP 
для отрицательного исхода (не выиграть награду), а второй массив — это список значений SHAP для положительного 
исхода (получить награду). Обычно мы думаем о прогнозах с точки зрения предсказания положительного результата, 
поэтому мы будем извлекать значения SHAP для положительных результатов (вытягивая shap_values[1]).   

Просмотр необработанных массивов утомительный, но пакет shap имеет хороший способ визуализации результатов.

```python
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)
```
Если вы внимательно посмотрите на код, где мы создали значения SHAP, вы заметите, что мы ссылаемся на деревья в shap.TreeExplainer(my_model). Но в пакете SHAP есть объяснения для каждого типа модели.

shap.DeepExplainer работает с моделями глубокого обучения.
shap.KernelExplainer работает со всеми моделями, хотя он медленнее других эксплейнеров и предлагает приблизительные, а не точные значения Shap.
Вот пример использования KernelExplainer для получения аналогичных результатов. Результаты не идентичны, поскольку 
KernelExplainer дает приблизительный результат. Но результаты говорят о том же.  

```python
# use Kernel SHAP to explain test set predictions
k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)
k_shap_values = k_explainer.shap_values(data_for_prediction)
shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)
```


