# Advanced Uses of SHAP Values
Резюме
Мы начали с изучения важности перестановок и графиков частичной зависимости для обзора того, чему научилась модель.

Затем мы узнали о значениях SHAP, чтобы разбить компоненты отдельных прогнозов.

Теперь мы расширим значения SHAP, чтобы увидеть, как агрегирование многих значений SHAP может дать более подробные альтернативы важности перестановки и графикам частичной зависимости.

Обзор ценностей SHAP
Значения Shap показывают, насколько данная функция изменила наш прогноз (по сравнению с тем, если бы мы сделали этот прогноз при некотором базовом значении этой функции).

Например, рассмотрим сверхпростую модель:
у=4*х1+2*х2
 
Если x1 принимает значение 2 вместо базового значения 0, то наше значение SHAP для x1 будет равно 8 (из 4 умноженных на 2).

Их труднее вычислить с помощью сложных моделей, которые мы используем на практике. Но благодаря некоторой алгоритмической хитрости значения Shap позволяют нам разложить любой прогноз на сумму эффектов каждого значения функции, что дает такой график:

Имгур

Ссылка на увеличенное изображение*

В дополнение к этой приятной разбивке для каждого прогноза библиотека Shap предлагает отличные визуализации групп значений Shap. Мы сосредоточимся на двух из этих визуализаций. Эти визуализации имеют концептуальное сходство с графиками важности перестановки и частичной зависимости. Таким образом, здесь соберутся несколько потоков из предыдущих упражнений.

Сводные сюжеты
Важность перестановки велика, потому что она создает простые числовые меры, чтобы увидеть, какие функции важны для модели. Это помогло нам легко проводить сравнения между функциями, и вы можете представить полученные графики нетехнической аудитории.

Но это не говорит вам, как каждая функция имеет значение. Если признак имеет среднюю важность перестановки, это может означать, что он имеет

большой эффект для нескольких предсказаний, но никакого эффекта в целом, или
средний эффект для всех предсказаний.
Сводные графики SHAP дают нам общее представление о важности функций и о том, что ими движет. Мы рассмотрим пример графика для футбольных данных:

Имгур

Этот сюжет состоит из множества точек. Каждая точка имеет три характеристики:

Вертикальное расположение показывает, какую функцию оно изображает.
Цвет показывает, была ли эта функция высокой или низкой для этой строки набора данных.
Горизонтальное расположение показывает, вызвал ли эффект этого значения более высокий или более низкий прогноз.
Например, точка в верхнем левом углу была для команды, забившей мало голов, что уменьшило прогноз на 0,25.

Некоторые вещи, которые вы должны быть в состоянии легко выбрать:

Модель игнорировала функции Red и Yellow & Red.
Обычно желтая карточка не влияет на прогноз, но есть крайний случай, когда высокое значение приводит к гораздо более низкому прогнозу.
Высокие значения забитых голов приводили к более высоким прогнозам, а низкие значения — к заниженным прогнозам.
Если вы будете искать достаточно долго, на этом графике будет много информации. Вы столкнетесь с некоторыми вопросами, чтобы проверить, как вы их читаете в упражнении.

Сводные графики в коде
Вы уже видели код для загрузки данных о футболе/футболе:
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')
y = (data['Man of the Match'] == "Yes")  # Convert from string "Yes"/"No" to binary
feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)
```

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# calculate shap values. This is what we will plot.
# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(val_X)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values[1], val_X)
```
Код не слишком сложный. Но есть несколько предостережений.

При построении графика мы вызываем shap_values[1]. Для задач классификации существует отдельный массив значений SHAP для каждого возможного результата. В этом случае мы индексируем, чтобы получить значения SHAP для прогноза «Истина».
Вычисление значений SHAP может быть медленным. Здесь это не проблема, потому что этот набор данных небольшой. Но вы должны быть осторожны, запуская их для построения графиков с наборами данных разумного размера. Исключением является использование модели xgboost, для которой SHAP имеет некоторые оптимизации и поэтому работает намного быстрее.
Это обеспечивает отличный обзор модели, но мы могли бы захотеть углубиться в одну функцию. Вот где в игру вступают графики вклада зависимостей SHAP.

Графики вклада зависимостей SHAP
Ранее мы использовали графики частичной зависимости, чтобы показать, как одна функция влияет на прогнозы. Они проницательны и актуальны для многих реальных случаев использования. Кроме того, с небольшим усилием их можно объяснить нетехнической аудитории.

Но многого они не показывают. Например, каково распределение эффектов? Является ли эффект наличия определенного 
значения довольно постоянным, или он сильно варьируется в зависимости от значений других свойств? Графики вклада 
зависимостей SHAP дают такое же понимание, как и PDP, но они добавляют гораздо больше деталей.   

Начните с фокусировки на форме, а через минуту мы вернемся к цвету. Каждая точка представляет строку данных. 
Расположение по горизонтали — это фактическое значение из набора данных, а расположение по вертикали показывает, как 
это значение повлияло на прогноз. Тот факт, что он наклонен вверх, говорит о том, что чем больше вы владеете мячом, 
тем выше прогноз модели для получения награды Лучшего игрока матча.   

Распространение предполагает, что другие функции должны взаимодействовать с процентом владения мячом. Например, 
здесь мы выделили две точки с одинаковыми показателями владения мячом. Это значение привело к увеличению одного 
прогноза и уменьшению другого прогноза.  

Для сравнения, простая линейная регрессия будет давать графики, которые представляют собой идеальные линии без этого разброса.

Это предполагает, что мы углубимся во взаимодействие, и графики включают цветовое кодирование, чтобы помочь в этом. 
В то время как основной тренд восходящий, вы можете визуально проверить, меняется ли он по цвету точки. 

Рассмотрим следующий очень узкий пример для конкретности.

Эти две точки пространственно выделяются тем, что находятся далеко от восходящего тренда. Они оба окрашены в фиолетовый цвет, что указывает на то, что команда забила один гол. Вы можете интерпретировать это так: «В целом, владение мячом увеличивает шансы команды на то, что их игрок получит награду». Но если они забивают только один гол, эта тенденция меняется на противоположную, и судьи могут оштрафовать их за то, что они так часто владеют мячом, если они забивают так мало.

За исключением этих нескольких выбросов, взаимодействие, обозначенное цветом, здесь не очень драматично. Но иногда это будет прыгать на вас.

Графики вклада зависимости в коде
Мы получаем график вклада зависимости с помощью следующего кода. Единственная строка, которая отличается от 
summary_plot, — это последняя строка. 

```python
import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(my_model)

# calculate shap values. This is what we will plot.
shap_values = explainer.shap_values(X)

# make plot.
shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index="Goal Scored")
```

Если вы не укажете аргумент дляinteraction_index, Шепли использует некоторую логику, чтобы выбрать тот, который может быть интересен.

Это не требовало написания большого количества кода. Но хитрость этих методов заключается в том, чтобы критически 
относиться к результатам, а не к написанию самого кода. 