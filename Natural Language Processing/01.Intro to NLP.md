# Intro to NLP (обработка естественного языка)
Данные бывают разных форм: отметки времени, показания датчиков, изображения, категориальные метки и многое другое. 
Но текст по-прежнему остается одним из самых ценных данных для тех, кто знает, как его использовать. 

В этом курсе обработки естественного языка (NLP) вы будете использовать ведущую библиотеку NLP (spaCy), чтобы взять 
на себя некоторые из наиболее важных задач при работе с текстом. 

К концу вы сможете использовать spaCy для:

- Базовая обработка текста и сопоставление с образцом
- Построение моделей машинного обучения с помощью текста
- Представление текста с вкраплениями слов, которые численно фиксируют значение слов и документов


Чтобы получить максимальную отдачу от этого курса, вам понадобится некоторый опыт в машинном обучении. Если у вас 
нет опыта работы с scikit-learn, посмотрите Введение в машинное обучение и промежуточное машинное обучение, чтобы 
изучить основы.  

### НЛП со спа-центром
spaCy - ведущая библиотека для НЛП, и она быстро стала одной из самых популярных фреймворков Python. Большинство 
людей находят его интуитивно понятным и снабженным отличной документацией. 

spaCy полагается на модели, которые зависят от языка и бывают разных размеров. Вы можете загрузить модель spaCy с 
помощью spacy.load. 

Например, вот как вы загрузите англоязычную модель.

```python
import spacy
nlp = spacy.load('en_core_web_sm')
```
Когда модель загружена, вы можете обрабатывать текст следующим образом:
```python
doc = nlp("Tea is healthy and calming, don't you think?")
```
Вы можете многое сделать с только что созданным объектом документа.

### Токенизация
Это возвращает объект документа, содержащий токены. Маркер - это единица текста в документе, например отдельные 
слова и знаки препинания. SpaCy разбивает сокращения, такие как «не делать», на два токена «делать» и «нет». Вы можете 
увидеть токены, просматривая документ.
```python
for token in doc:
    print(token)
```
Итерация по документу дает вам объекты-токены. Каждый из этих токенов содержит дополнительную информацию. В 
большинстве случаев важными являются token.lemma_ и token.is_stop. 

### Предварительная обработка текста
Есть несколько типов предварительной обработки, чтобы улучшить то, как мы моделируем с помощью слов. Первый - 
«лемматизация». «Лемма» слова - это его основная форма. Например, «прогулка» - это лемма слова «ходьба». Итак, если 
вы лемматизируете слово «ходьба», вы превратите его в «ходьба».  

Также часто удаляются игнорируемые слова. Стоп-слова - это слова, которые часто встречаются в языке и не содержат 
много информации. Английские стоп-слова включают «the», «is», «and», «but», «not». 

С токеном spaCy token.lemma_ возвращает лемму, а token.is_stop возвращает логическое значение True, если токен 
является стоп-словом (и False в противном случае). 

```python
print(f"Token \t\tLemma \t\tStopword".format('Token', 'Lemma', 'Stopword'))
print("-"*40)
for token in doc:
    print(f"{str(token)}\t\t{token.lemma_}\t\t{token.is_stop}")
```
Почему важны леммы и определение игнорируемых слов? В языковых данных много шума, смешанного с информативным 
содержанием. В предложении выше важны слова чай, полезный и успокаивающий. Удаление стоп-слов может помочь 
прогнозной модели сосредоточиться на релевантных словах. Аналогичным образом помогает лемматизация, комбинируя 
несколько форм одного и того же слова в одну базовую форму («успокаивающий», «успокаивающий», «успокаивающий» - все 
изменится на «успокаивающий»).    

Однако лемматизация и отбрасывание игнорируемых слов может привести к ухудшению работы ваших моделей. Поэтому вам 
следует рассматривать эту предварительную обработку как часть процесса оптимизации гиперпараметров. 

### Соответствие шаблону
Другая распространенная задача НЛП - сопоставление лексем или фраз внутри фрагментов текста или целых документов. 
Сопоставление с образцом можно выполнять с помощью регулярных выражений, но возможности сопоставления spaCy, как 
правило, проще в использовании.  

Чтобы сопоставить отдельные токены, вы создаете сопоставление. Если вы хотите сопоставить список терминов, проще и 
эффективнее использовать PhraseMatcher. Например, если вы хотите узнать, где в тексте отображаются различные модели 
смартфонов, вы можете создать шаблоны для названий интересующих моделей. Сначала вы создаете сам PhraseMatcher.  

```python
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab, attr='LOWER')
```
Сопоставитель создается с использованием словаря вашей модели. Здесь мы используем небольшую английскую модель, 
которую вы загрузили ранее. Установка attr = 'LOWER' будет соответствовать фразам в тексте в нижнем регистре. Это 
обеспечивает нечувствительность к регистру.  

Затем вы создаете список терминов, которые должны совпадать в тексте. Средству сопоставления фраз нужны шаблоны как 
объекты документа. Самый простой способ получить их - составить список с использованием модели nlp. 

```python
terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']
patterns = [nlp(text) for text in terms]
matcher.add("TerminologyList", patterns)
```
Затем вы создаете документ из текста для поиска и используете средство сопоставления фраз, чтобы найти, где в тексте 
встречаются термины. 
```python
# Заимствовано с https://daringfireball.net/linked/2019/09/21/patel-11-pro
text_doc = nlp("Glowing review overall, and some really interesting side-by-side "
               "photography tests pitting the iPhone 11 Pro against the "
               "Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.") 
matches = matcher(text_doc)
print(matches)
```
[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]



Здесь совпадения представляют собой кортеж из идентификатора совпадения и позиций начала и конца фразы.

