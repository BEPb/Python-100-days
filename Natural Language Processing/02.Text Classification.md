# Text Classification
### Классификация текста с помощью SpaCy
Распространенной задачей в НЛП является классификация текста. Это «классификация» в обычном смысле машинного 
обучения, и она применяется к тексту. Примеры включают обнаружение спама, анализ настроений и маркировку запросов 
клиентов.  

В этом руководстве вы изучите классификацию текста с помощью spaCy. Классификатор обнаруживает спам-сообщения, что 
является обычной функцией большинства почтовых клиентов. Вот обзор данных, которые вы будете использовать:  
```python
import pandas as pd

# Loading the spam data
# ham is the label for non-spam messages
spam = pd.read_csv('../input/nlp-course/spam.csv')
spam.head(10)
```
### Мешок слов
Модели машинного обучения не учатся на необработанных текстовых данных. Вместо этого вам нужно преобразовать текст 
во что-то числовое. 

Простейшее распространенное представление - это вариант горячего кодирования. Вы представляете каждый документ как 
вектор частотности терминов для каждого термина в словаре. Словарь строится из всех токенов (терминов) в корпусе 
(коллекции документов).  

В качестве примера возьмем предложения «Чай - это жизнь. Чай - это любовь». и «Чай полезен, успокаивает и 
восхитителен». как наш корпус. Тогда словарь будет {"чай", "есть", "жизнь", "любовь", "здоровый", "успокаивающий" 
"и", "вкусный"} (без знаков препинания).  

Для каждого документа подсчитайте, сколько раз встречается термин, и поместите этот счет в соответствующий элемент 
вектора. В первом предложении дважды встречается слово «чай», и это первая позиция в нашем словаре, поэтому мы 
помещаем цифру 2 в первый элемент вектора. Тогда наши предложения как векторы выглядят как  

v1v2 = [22110000] = [11001111]
 
Это называется представлением мешка слов. Вы можете видеть, что документы с похожими терминами будут иметь похожие 
векторы. Словари часто содержат десятки тысяч терминов, поэтому эти векторы могут быть очень большими. 

Другое распространенное представление - TF-IDF (частота термина - обратная частота документа). TF-IDF похож на мешок 
слов, за исключением того, что количество каждого термина масштабируется по частоте термина в корпусе. Использование 
TF-IDF потенциально может улучшить ваши модели. Здесь он вам не понадобится. Не стесняйтесь поискать это!  

### Создание модели мешка слов
Если у вас есть документы в виде пакета слов, вы можете использовать эти векторы в качестве входных данных для любой 
модели машинного обучения. spaCy обрабатывает пакет преобразования слов и строит для вас простую линейную модель с 
помощью класса TextCategorizer.  

TextCategorizer - это канал spaCy. Каналы - это классы для обработки и преобразования токенов. Когда вы создаете 
модель spaCy с помощью nlp = spacy.load ('en_core_web_sm'), существуют каналы по умолчанию, которые выполняют часть 
тегов речи, распознавания сущностей и других преобразований. Когда вы пропускаете текст через модель doc = nlp 
(«Здесь немного текста»), выходные данные каналов прикрепляются к токенам в объекте документа. Леммы для token.
lemma_ взяты из одного из этих каналов.    

Вы можете удалить или добавить трубы к моделям. Что мы здесь сделаем, так это создадим пустую модель без каких-либо 
каналов (кроме токенизатора, поскольку все модели всегда имеют токенизатор). Затем мы создадим канал TextCategorizer 
и добавим его в пустую модель.  

```python
import spacy

# Create an empty model
nlp = spacy.blank("en")

# Add the TextCategorizer to the empty model
textcat = nlp.add_pipe("textcat")
```
Затем мы добавим метки к модели. Здесь «ветчина» - это настоящие сообщения, «спам» - это спам-сообщения.
```python
# Add labels to text classifier
textcat.add_label("ham")
textcat.add_label("spam")
```
### Обучение модели классификатора текста
Затем вы преобразуете метки в данных в форму, требуемую TextCategorizer. Для каждого документа вы создадите словарь 
логических значений для каждого класса. 

Например, если текст - «ветчина», нам нужен словарь {'ham': True, 'spam': False}. Модель ищет эти ярлыки в другом 
словаре с ключом «коты». 
```python
train_texts = spam['text'].values
train_labels = [{'cats': {'ham': label == 'ham',
                          'spam': label == 'spam'}} 
                for label in spam['label']]
```
Затем объединяем тексты и надписи в единый список.
```python
train_data = list(zip(train_texts, train_labels))
train_data[:3]
```
Теперь вы готовы обучать модель. Сначала создайте оптимизатор, используя nlp.begin_training (). spaCy использует 
этот оптимизатор для обновления модели. Как правило, более эффективно обучать модели небольшими партиями. spaCy 
предоставляет функцию мини-пакетов, которая возвращает генератор, выдающий мини-пакеты для обучения. Наконец, 
мини-пакеты разделяются на тексты и метки, которые затем используются с nlp.update для обновления параметров модели.   
```python
from spacy.util import minibatch
from spacy.training.example import Example

spacy.util.fix_random_seed(1)
optimizer = nlp.begin_training()

# Create the batch generator with batch size = 8
batches = minibatch(train_data, size=8)
# Iterate through minibatches
for batch in batches:
    # Each batch is a list of (text, label) 
    for text, labels in batch:
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, labels)
        nlp.update([example], sgd=optimizer)
```
Это всего лишь один обучающий цикл (или эпоха) по данным. Для модели обычно требуется несколько эпох. Используйте 
другой цикл для большего количества эпох и, при желании, перетасуйте данные обучения в начале каждого цикла. 

```python
import random

random.seed(1)
spacy.util.fix_random_seed(1)
optimizer = nlp.begin_training()

losses = {}
for epoch in range(10):
    random.shuffle(train_data)
    # Create the batch generator with batch size = 8
    batches = minibatch(train_data, size=8)
    # Iterate through minibatches
    for batch in batches:
        for text, labels in batch:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, labels)
            nlp.update([example], sgd=optimizer, losses=losses)
    print(losses)
```
### Прогнозы
Теперь, когда у вас есть обученная модель, вы можете делать прогнозы с помощью метода pred (). Входной текст 
необходимо токенизировать с помощью nlp.tokenizer. Затем вы передаете токены методу прогнозирования, который 
возвращает оценки. Баллы - это вероятность того, что введенный текст принадлежит классам.  

```python
texts = ["Are you ready for the tea party????? It's gonna be wild",
         "URGENT Reply to this message for GUARANTEED FREE TEA" ]
docs = [nlp.tokenizer(text) for text in texts]
    
# Use textcat to get the scores for each doc
textcat = nlp.get_pipe('textcat')
scores = textcat.predict(docs)

print(scores)
```
Баллы используются для прогнозирования одного класса или метки путем выбора метки с наибольшей вероятностью. Вы 
получаете индекс наивысшей вероятности с помощью scores.argmax, а затем используете индекс, чтобы получить строку 
метки из textcat.labels.  
```python
# From the scores, find the label with the highest score/probability
predicted_labels = scores.argmax(axis=1)
print([textcat.labels[label] for label in predicted_labels])
```
Оценка модели проста, если у вас есть прогнозы. Чтобы измерить точность, вычислите, сколько правильных прогнозов 
сделано на некоторых тестовых данных, разделенное на общее количество прогнозов. 