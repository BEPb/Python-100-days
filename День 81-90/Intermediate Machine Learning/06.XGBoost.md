###XGBoost
В этом руководстве вы узнаете, как создавать и оптимизировать модели с усилением градиента. Этот метод доминирует 
во многих соревнованиях Kaggle и позволяет получать самые современные результаты на различных наборах данных.

###Вступление
На протяжении большей части этого курса вы делали прогнозы с помощью метода случайного леса, который обеспечивает 
лучшую производительность, чем одно дерево решений, просто путем усреднения прогнозов многих деревьев решений.

Мы называем метод случайного леса «методом ансамбля». По определению, ансамблевые методы объединяют прогнозы 
нескольких моделей (например, нескольких деревьев в случае случайных лесов).

Далее мы узнаем о другом методе ансамбля, называемом усилением градиента.

###Повышение градиента
Повышение градиента - это метод, который проходит циклы для итеративного добавления моделей в ансамбль.

Он начинается с инициализации ансамбля одной моделью, прогнозы которой могут быть довольно наивными. (Даже если его прогнозы совершенно неточны, последующие добавления в ансамбль исправят эти ошибки.)

Затем мы начинаем цикл:

- Во-первых, мы используем текущий ансамбль для создания прогнозов для каждого наблюдения в наборе данных. Чтобы 
сделать прогноз, мы добавляем прогнозы от всех моделей в ансамбле.
- Эти прогнозы используются для вычисления функции потерь (например, среднеквадратичной ошибки).
- Затем мы используем функцию потерь, чтобы соответствовать новой модели, которая будет добавлена ​​в ансамбль. В 
  частности, мы определяем параметры модели, чтобы добавление этой новой модели в ансамбль уменьшило потери. (Боковое примечание: «градиент» в «усилении градиента» относится к тому факту, что мы будем использовать градиентный спуск для функции потерь для определения параметров в этой новой модели.)
- Наконец, мы добавляем новую модель в ансамбль, и ...
... повторить!


###Пример
Мы начинаем с загрузки данных обучения и проверки в X_train, X_valid, y_train и y_valid.
```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Select subset of predictors
cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']
X = data[cols_to_use]

# Select target
y = data.Price

# Separate data into training and validation sets
X_train, X_valid, y_train, y_valid = train_test_split(X, y)
```
В этом примере вы будете работать с библиотекой XGBoost. XGBoost означает экстремальное повышение градиента, 
которое  представляет собой реализацию повышения градиента с несколькими дополнительными функциями, ориентированными 
на  производительность и скорость. (В Scikit-learn есть другая версия повышения градиента, но у XGBoost есть 
некоторые технические преимущества.)

В  следующей ячейке кода мы импортируем scikit-learn API для XGBoost (xgboost.XGBRegressor). Это позволяет нам 
построить  и подогнать модель так же, как в scikit-learn. Как вы увидите в выходных данных, класс XGBRegressor имеет 
множество настраиваемых параметров - вы скоро узнаете о них!

```python
from xgboost import XGBRegressor

my_model = XGBRegressor()
my_model.fit(X_train, y_train)
```


Мы также делаем прогнозы и оцениваем модель.

```python
from sklearn.metrics import mean_absolute_error

predictions = my_model.predict(X_valid)
print("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_valid)))
```
Средняя абсолютная ошибка: 239435.01260125183

###Настройка параметров
XGBoost имеет несколько параметров, которые могут существенно повлиять на точность и скорость обучения. Первые 
параметры, которые вам следует понять:

n_estimators
n_estimators указывает, сколько раз нужно пройти цикл моделирования, описанный выше. Он равен количеству моделей, которые мы включаем в ансамбль.

- Слишком низкое значение приводит к недостаточной подгонке, что приводит к неточным прогнозам как на данных обучения,
так и на данных тестирования.
- Слишком высокое значение приводит к переобучению, что приводит к точным прогнозам на обучающих данных, но к 
  неточным прогнозам на тестовых данных (это то, что нас волнует).
Типичные значения находятся в диапазоне от 100 до 1000, хотя это во многом зависит от параметра learning_rate, 
  обсуждаемого ниже.

Вот код для установки количества моделей в ансамбле:

```python
my_model = XGBRegressor(n_estimators=500)
my_model.fit(X_train, y_train)
```

Early_stopping_rounds
Early_stopping_rounds предлагает способ автоматического поиска идеального значения для n_estimators. Ранняя 
остановка приводит к тому, что модель прекращает итерацию, когда оценка валидации перестает улучшаться, даже если 
мы не находимся в жесткой  остановке для n_estimators. Разумно установить высокое значение для n_estimators, а затем 
использовать early_stopping_rounds, чтобы найти оптимальное время для остановки итерации.

Поскольку случайная вероятность  иногда приводит к единственному раунду, в котором оценки валидации не улучшаются, 
вам необходимо указать число,  указывающее, сколько раундов прямого ухудшения необходимо допустить, прежде чем 
останавливаться. Установка  early_stopping_rounds = 5 - разумный выбор. В этом случае мы останавливаемся после 5 
раундов подряд ухудшения оценок валидации.

При использовании  Early_stopping_rounds вам также необходимо выделить некоторые данные для расчета оценок валидации 
- это делается путем установки параметра eval_set.

Мы можем изменить  приведенный выше пример, чтобы включить раннюю остановку:

```python
my_model = XGBRegressor(n_estimators=500)
my_model.fit(X_train, y_train, 
             early_stopping_rounds=5, 
             eval_set=[(X_valid, y_valid)],
             verbose=False)
```
Если позже вы захотите подогнать модель ко всем вашим данным, установите n_estimators на любое значение, которое вы 
сочтете оптимальным при запуске с ранней остановкой.

Learning_rate
Вместо того, чтобы получать прогнозы, просто складывая прогноз ионов из каждой компонентной модели, мы можем 
умножить прогнозы каждой модели на небольшое число (известное как скорость обучения) перед их добавлением.

Это означает, что каждое дерево, которое мы добавляем в ансамбль, помогает нам меньше. Итак, мы можем установить 
более высокое значение для n_estimators без переобучения. Если мы используем раннюю остановку, соответствующее 
количество деревьев будет определено автоматически.

В целом, небольшая скорость обучения и большое количество оценщиков позволят получить более точные модели XGBoost, 
хотя обучение модели также займет больше времени, поскольку она выполняет больше итераций в цикле. По умолчанию 
XGBoost устанавливает learning_rate = 0,1.

Изменение приведенного выше примера для изменения скорости обучения дает следующий код:

```python
my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)
my_model.fit(X_train, y_train, 
             early_stopping_rounds=5, 
             eval_set=[(X_valid, y_valid)], 
             verbose=False)
```
В больших наборах данных, где время выполнения имеет значение, вы можете использовать параллелизм для более 
быстрого построения моделей. Обычно параметр n_jobs устанавливается равным количеству ядер на вашем компьютере. Для 
небольших наборов данных это не поможет.

Полученная модель ничуть не лучше, поэтому микрооптимизация времени подгонки обычно не более чем отвлекает. Но это 
полезно в больших наборах данных, где в противном случае вы бы долго ожидали выполнения команды подгонки.

Вот модифицированный пример:

```python
my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
my_model.fit(X_train, y_train, 
             early_stopping_rounds=5, 
             eval_set=[(X_valid, y_valid)], 
             verbose=False)
```
Заключение
XGBoost - это ведущая  программная библиотека для работы со стандартными табличными данными (тип данных, которые вы 
храните в Pandas DataFrames,  в отличие от более экзотических типов данных, таких как изображения и видео). 
Тщательная настройка параметров позволяет обучать высокоточные модели. 




























