# Identifying Bias in AI

### Введение
Машинное обучение (МО) может улучшить жизнь, но оно также может быть источником вреда. Заявки на отмывание денег 
дискриминируют людей по признаку расы, пола, религии, социально-экономического положения и других категорий.  

В этом руководстве вы узнаете о предвзятости, которая относится к негативным, нежелательным последствиям приложений 
ML, особенно если последствия несоразмерно затрагивают определенные группы. 

Мы рассмотрим шесть различных типов предвзятости, которые могут повлиять на любое приложение ML. Затем вы примените 
свои новые знания на практике в практическом упражнении, в котором вы определите предвзятость в реальном сценарии. 

### Предвзятость сложна
Многие практики машинного обучения знакомы с «предвзятыми данными» и концепцией «мусор на входе, мусор на выходе». 
Например, если вы обучаете чат-бота, используя набор данных, содержащий антисемитские онлайн-разговоры («мусор на 
входе»), чат-бот, скорее всего, будет делать антисемитские замечания («мусор на выходе»). В этом примере подробно 
описывается важный тип предвзятости (называемый историческим предубеждением, как вы увидите ниже), который следует 
распознавать и устранять.

Это не единственный способ, которым предвзятость может разрушить приложения ML.

Систематическая ошибка в данных сложна. Неверные данные также могут привести к смещению представления (рассмотрено 
далее в этом руководстве), если группа недостаточно представлена в обучающих данных. Например, при обучении 
системы обнаружения лиц, если обучающие данные содержат в основном людей со светлым оттенком кожи, она не будет 
работать хорошо для пользователей с более темным оттенком кожи. Третий тип систематической ошибки, которая может 
возникнуть из-за обучающих данных, называется систематической ошибкой измерения, о которой вы узнаете ниже.    

И это не только предвзятые данные, которые могут привести к недобросовестным приложениям ML: как вы узнаете, 
предвзятость также может быть результатом того, как определяется модель ML, от того, как модель сравнивается с 
другими моделями, и от того, как что обычные пользователи интерпретируют окончательные результаты модели. Вред может 
быть нанесен из любого места в процессе ОД.   

### Шесть типов предвзятости
Как только мы узнаем о различных типах предвзятости, мы с большей вероятностью обнаружим их в проектах ML. Кроме 
того, имея общий словарный запас, мы можем вести плодотворные разговоры о том, как смягчить (или уменьшить) 
предвзятость.

Мы будем внимательно следить за исследовательской работой начала 2020 года, в которой описываются шесть различных 
типов предвзятости. 

### Исторический уклон
Историческая предвзятость возникает, когда состояние мира, в котором были сгенерированы данные, ошибочно.

По состоянию на 2020 год только 7,4% генеральных директоров Fortune 500 составляют женщины. Исследования показали, 
что компании с женщинами-генеральными или финансовыми директорами, как правило, более прибыльны, чем компании с 
мужчинами на той же должности, что позволяет предположить, что женщины предъявляют более высокие требования к найму, 
чем мужчины. Чтобы исправить это, мы могли бы рассмотреть возможность исключения человеческого участия и 
использования ИИ, чтобы сделать процесс найма более справедливым. Но это может оказаться непродуктивным, если для 
обучения модели используются данные о прошлых решениях о найме, потому что модель, скорее всего, научится 
демонстрировать те же предубеждения, которые присутствуют в данных.

### Предвзятость представления
Смещение репрезентации возникает при создании наборов данных для обучения модели, если эти наборы данных плохо 
представляют людей, которых модель будет обслуживать.

Данные, собранные с помощью приложений для смартфонов, будут недостаточно репрезентативными для групп, которые с 
меньшей вероятностью будут владеть смартфонами. Например, при сборе данных в США лица старше 65 лет будут 
недостаточно представлены. Если данные будут использоваться для разработки городской транспортной системы, это будет 
иметь катастрофические последствия, поскольку у пожилых людей есть важные потребности в обеспечении доступности 
системы.

### Смещение измерения
Систематическая ошибка измерения возникает, когда точность данных варьируется в зависимости от группы. Это может 
произойти при работе с прокси-переменными (переменными, заменяющими переменную, которую нельзя измерить напрямую), 
если качество прокси различается в разных группах.   

Ваша местная больница использует модель для выявления пациентов с высоким риском до того, как у них разовьются 
серьезные заболевания, на основе такой информации, как прошлые диагнозы, лекарства и демографические данные. Модель 
использует эту информацию для прогнозирования затрат на здравоохранение, идея заключается в том, что пациенты с 
более высокими затратами, вероятно, соответствуют пациентам с высоким риском. Несмотря на то, что модель специально 
исключает расу, она, похоже, демонстрирует расовую дискриминацию: алгоритм с меньшей вероятностью выберет подходящих 
чернокожих пациентов. Как это может быть? Это связано с тем, что стоимость использовалась в качестве косвенного 
показателя риска, а взаимосвязь между этими переменными зависит от расы: чернокожие пациенты сталкиваются с более 
высокими барьерами для получения помощи, меньше доверяют системе здравоохранения и, следовательно, имеют более 
низкие медицинские расходы в среднем, когда по сравнению с не чернокожими пациентами с теми же состояниями здоровья. 


### Смещение агрегации
Смещение агрегации возникает, когда группы объединяются ненадлежащим образом, в результате чего модель не работает 
хорошо для какой-либо группы или хорошо работает только для группы большинства. (Часто это не проблема, но чаще 
всего возникает в медицинских приложениях.)  

У латиноамериканцев более высокий уровень диабета и осложнений, связанных с диабетом, чем у тех, у кого нет 
n-латиноамериканские белые. При создании ИИ для диагностики или мониторинга диабета важно сделать систему 
чувствительной к этим этническим различиям, либо включив этническую принадлежность в качестве признака данных, либо 
создав отдельные модели для разных этнических групп.   

### Предвзятость оценки
Смещение оценки возникает при оценке модели, если контрольные данные (используемые для сравнения модели с другими 
моделями, выполняющими аналогичные задачи) не представляют совокупность, которую модель будет обслуживать. 

В документе «Гендерные оттенки» было обнаружено, что два широко используемых эталонных набора данных для анализа 
лица (IJB-A и Adience) в основном состоят из субъектов со светлой кожей (79,6% и 86,2% соответственно). Коммерческий 
ИИ для гендерной классификации показал самые современные результаты в этих тестах, но имел непропорционально высокий 
уровень ошибок с цветными людьми.   

### Предвзятость развертывания
Предвзятость развертывания возникает, когда проблема, для решения которой предназначена модель, отличается от того, 
как она фактически используется. Если конечные пользователи не используют модель так, как она предназначена, нет 
гарантии, что модель будет работать хорошо.   

Система уголовного правосудия использует инструменты для прогнозирования вероятности рецидива преступного поведения 
осужденного преступника. Прогнозы не предназначены для судей при определении соответствующих наказаний во время 
вынесения приговора.  

Мы можем визуально представить эти различные типы предвзятости, возникающие на разных этапах рабочего процесса 
машинного обучения: 

визуальное представление типов предвзятости

Обратите внимание, что они не исключают друг друга: то есть приложение ML может легко страдать от более чем одного 
типа предвзятости. Например, как описывает Рэйчел Томас в недавнем исследовательском докладе, приложения машинного 
обучения в носимых фитнес-устройствах могут страдать от:  

- Смещение представления (если набор данных, используемый для обучения моделей, исключает более темные тона кожи),
- Смещение измерения (если измерительное устройство показывает снижение производительности при темных тонах кожи) и
- Предвзятость оценки (если набор данных, используемый для сравнения модели, исключает более темные тона кожи).
