Underfitting and Overfitting
###Недооснащение и переобучение
 В конце этого шага вы поймете концепции недостаточной и избыточной подгонки и сможете применить эти идеи, чтобы 
 сделать свои модели более точными.

Экспериментируйте с разными моделями
 Теперь, когда у вас есть надежный способ измерения точности модели, вы можете поэкспериментировать с 
альтернативными моделями и посмотреть, какие из них дают наилучшие прогнозы. Но какие у вас есть альтернативы моделям?

 В документации scikit-learn вы можете увидеть, что модель дерева решений имеет много опций (больше, чем вам нужно 
  или понадобится в течение длительного времени). Наиболее важные параметры определяют глубину дерева. Вспомните из 
  первого урока этого курса, что глубина дерева - это мера того, сколько расщеплений оно делает перед тем, как 
 прийти к прогнозу.
 
На практике дерево нередко имеет 10 секций между верхним уровнем (всеми домами) и листом. По мере того, как дерево 
становится глубже, набор данных разрезается на листья с меньшим количеством домов. Если в дереве было только одно 
разбиение, оно делит данные на 2 группы. Если каждую группу снова разделить, мы получим 4 группы домов. Повторное 
разделение каждого из них приведет к созданию 8 групп. Если мы продолжим удваивать количество групп, добавляя 
больше разделений на каждом уровне, к тому времени, когда мы дойдем до 10-го уровня, у нас будет 210 групп домов. 
Это 1024 листа.

Когда мы делим дома на множество листьев, у нас также становится меньше домов на каждом листе. Листья с очень 
небольшим количеством домов будут давать прогнозы, довольно близкие к фактическим значениям этих домов, но они 
могут давать очень ненадежные прогнозы для новых данных (потому что каждый прогноз основан только на нескольких домах).

Это явление, называемое переобучением, когда модель почти идеально соответствует обучающим данным, но плохо 
справляется с проверкой и другими новыми данными. С другой стороны, если мы сделаем дерево очень мелким, оно не 
разделит дома на очень отдельные группы.

В крайнем случае, если дерево делит дома только на 2 или 4, в каждой группе по-прежнему будет множество домов. 
Результирующие прогнозы могут быть далекими для большинства домов, даже в обучающих данных (и это будет плохо при 
проверке по той же причине). Когда модель не может уловить важные различия и закономерности в данных, поэтому она 
плохо работает даже в обучающих данных, это называется недостаточной подгонкой.

Поскольку мы заботимся о точности новых данных, которые мы оцениваем на основе наших данных проверки, мы хотим 
найти золотую середину между недостаточным и избыточным соответствием. 

Пример
Есть несколько альтернатив для управления глубиной дерева, и многие позволяют, чтобы некоторые маршруты через 
дерево имели большую глубину, чем другие маршруты. Но аргумент max_leaf_nodes предоставляет очень разумный способ 
контролировать переоснащение и недостаточное. Чем больше листьев мы позволяем модели сделать, тем больше мы 
перемещаемся от области недостаточной подгонки к области переобучения.

Мы можем использовать служебную функцию, чтобы сравнить оценки MAE для разных значений max_leaf_nodes:

```python
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)
```
Данные загружаются в train_X, val_X, train_y и val_y с использованием кода, который вы уже видели (и который вы уже 
написали).
```python
# Data Loading Code Runs At This Point
import pandas as pd
    
# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing values
filtered_melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = filtered_melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = filtered_melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
```
Мы можем использовать цикл for для сравнения точности моделей, построенных с разными значениями max_leaf_nodes. 
```python
# compare MAE with differing values of max_leaf_nodes
# сравним MAE с разными значениями max_leaf_nodes
for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))
```
Из перечисленных вариантов 500 - оптимальное количество листочков.

Заключение¶
Вывод: модели могут пострадать от:
Переобучение: фиксация ложных паттернов, которые больше не повторится в будущем, что приводит к менее точным 
прогнозам, или


Недостаточное соответствие: неспособность уловить релевантные закономерности, что опять же приводит к менее точным 
прогнозам.
Мы  используем данные проверки, которые не используются при обучении модели, для измерения точности модели-кандидата.
Это позволяет нам опробовать множество моделей-кандидатов и сохранить лучшую.

###Резюме
Вы построили свою первую модель, и теперь пора оптимизировать размер дерева, чтобы делать более точные прогнозы. 
Запустите эту ячейку, чтобы настроить среду кодирования там, где остановился предыдущий шаг.
```python
# Code you have previously used to load data
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor


# Path of the file to read
iowa_file_path = '../input/home-data-for-ml-course/train.csv'

home_data = pd.read_csv(iowa_file_path)
# Create target object and call it y
y = home_data.SalePrice
# Create X
features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
X = home_data[features]

# Split into validation and training data
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

# Specify Model
iowa_model = DecisionTreeRegressor(random_state=1)
# Fit Model
iowa_model.fit(train_X, train_y)

# Make validation predictions and calculate mean absolute error
val_predictions = iowa_model.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE: {:,.0f}".format(val_mae))

# Set up code checking
from learntools.core import binder
binder.bind(globals())
from learntools.machine_learning.ex5 import *
print("\nSetup complete")
```


###Упражнения
Вы можете сами написать функцию get_mae. А пока мы его поставим. Это та же функция, о которой вы читали в 
предыдущем уроке. Просто запустите ячейку ниже. 
```python
def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)
```
###Шаг 1. Сравните разные размеры деревьев¶
Напишите цикл, который пробует следующие значения max_leaf_nodes из набора возможных значений.

Вызовите функцию get_mae для каждого значения max_leaf_nodes. Сохраните вывод таким образом, чтобы вы могли выбрать 
значение max_leaf_nodes, которое дает наиболее точную модель для ваших данных.
```python
candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]
# Написать цикл, чтобы найти идеальный размер дерева из кандидата_max_leaf_nodes
# Вот краткое решение с пониманием dict.
# В уроке дается пример того, как это сделать с помощью явного цикла.
scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}

# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)
best_tree_size = min(scores, key=scores.get)
```
###Шаг 2: подогнать модель с использованием всех данных¶
Вы знаете лучший размер дерева. Если бы вы собирались применить эту модель на практике, вы бы сделали ее еще более 
точной, используя все данные и сохранив размер дерева. То есть вам не нужно хранить данные проверки сейчас, когда 
вы приняли все решения по моделированию.
```python
# Fill in argument to make optimal size and uncomment

# Fit the model with best_tree_size. Fill in argument to make optimal size
final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)

# fit the final model and uncomment the next two lines
final_model.fit(X, y)
```


