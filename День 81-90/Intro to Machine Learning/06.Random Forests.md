Random Forests
###Случайные леса
Вступление
Деревья решений оставляют вас перед трудным решением. Глубокое дерево с большим количеством листьев подойдет, 
потому что каждый прогноз основан на исторических данных только по нескольким домам на его листе. Но неглубокое 
дерево с небольшим количеством листьев будет плохо работать, потому что не сможет уловить столько различий в 
необработанных данных.

Даже самые сложные современные методы моделирования сталкиваются с этим противоречием между недостаточной и 
чрезмерной подгонкой. Но у многих моделей есть умные идеи, которые могут привести к повышению производительности. В 
качестве примера рассмотрим случайный лес.

В случайном лесу используется много деревьев, и он делает прогноз, усредняя прогнозы каждого дерева компонентов. 
Как правило, оно имеет гораздо лучшую точность прогнозирования, чем одно дерево решений, и хорошо работает с 
параметрами по умолчанию. Если вы продолжите моделирование, вы сможете изучить больше моделей с еще большей 
производительностью, но многие из них чувствительны к получению правильных параметров.

### Пример
Вы уже видели код для загрузки данных несколько раз. В конце загрузки данных у нас есть следующие переменные:

```
train_X
val_X
train_y
val_y
```

```python
import pandas as pd
    
# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing values
melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
```
Мы строим модель случайного леса аналогично тому, как мы строили дерево решений в scikit-learn - на этот раз с 
использованием класса RandomForestRegressor вместо DecisionTreeRegressor.
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```
Заключение
Вероятно, есть возможности для дальнейшего улучшения, но это большое улучшение по сравнению с ошибкой дерева лучших 
решений, равной 250 000. Есть параметры, которые позволяют вам изменять производительность случайного леса так же, 
как мы изменили максимальную глубину единственного дерева решений. Но одна из лучших особенностей моделей 
случайного леса заключается в том, что они обычно работают нормально даже без этой настройки.


### Резюме
Вот код, который вы написали.
```python
# Code you have previously used to load data
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor


# Path of the file to read
iowa_file_path = '../input/home-data-for-ml-course/train.csv'

home_data = pd.read_csv(iowa_file_path)
# Create target object and call it y
y = home_data.SalePrice
# Create X
features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
X = home_data[features]

# Split into validation and training data
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)

# Specify Model
iowa_model = DecisionTreeRegressor(random_state=1)
# Fit Model
iowa_model.fit(train_X, train_y)

# Make validation predictions and calculate mean absolute error
val_predictions = iowa_model.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE when not specifying max_leaf_nodes: {:,.0f}".format(val_mae))

# Using best value for max_leaf_nodes
iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)
iowa_model.fit(train_X, train_y)
val_predictions = iowa_model.predict(val_X)
val_mae = mean_absolute_error(val_predictions, val_y)
print("Validation MAE for best value of max_leaf_nodes: {:,.0f}".format(val_mae))


# Set up code checking
from learntools.core import binder
binder.bind(globals())
from learntools.machine_learning.ex6 import *
print("\nSetup complete")
```
### Упражнения¶
Наука о данных не всегда так проста. Но замена дерева решений на случайный лес будет легкой победой.

### Шаг 1: используйте случайный лес
```python
from sklearn.ensemble import RandomForestRegressor

# Define the model. Set random_state to 1
rf_model = RandomForestRegressor()

# fit your model
rf_model.fit(train_X, train_y)

# Calculate the mean absolute error of your Random Forest model on the validation data
rf_val_predictions = rf_model.predict(val_X)
rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)

print("Validation MAE for Random Forest Model: {}".format(rf_val_mae))
```



