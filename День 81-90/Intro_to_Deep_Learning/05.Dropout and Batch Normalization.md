# Dropout and Batch Normalization
### производство
В мире глубокого обучения есть нечто большее, чем просто плотные слои. Есть десятки видов слоев, которые вы можете 
добавить в модель. (Попробуйте просмотреть документацию Keras, чтобы найти образец!) Некоторые из них похожи на 
плотные слои и определяют связи между нейронами, а другие могут выполнять предварительную обработку или 
преобразования других видов.   

В этом уроке мы узнаем о двух типах специальных слоев, не содержащих сами нейроны, но добавляющих некоторые функции, 
которые иногда могут принести пользу модели по-разному. Оба обычно используются в современных архитектурах.  

### Выбывать
Первый из них - это «слой отсева», который может помочь исправить переобучение.

В прошлом уроке мы говорили о том, как переобучение вызвано ложными шаблонами обучения сети в обучающих данных. 
Чтобы распознать эти ложные паттерны, сеть часто будет полагаться на очень специфические комбинации весов, своего 
рода «заговор» весов. Поскольку они настолько специфичны, они имеют тенденцию быть хрупкими: уберите одну, и заговор 
развалится.   

Это идея отсева. Чтобы разрушить эти заговоры, мы случайным образом удаляем некоторую часть входных единиц слоя на 
каждом этапе обучения, что значительно усложняет сети изучение этих ложных шаблонов в обучающих данных. Вместо этого 
он должен искать широкие общие закономерности, веса которых имеют тенденцию быть более устойчивыми.  

Анимация сети, циклически перебирающей различные конфигурации случайного отключения.
Здесь между двумя скрытыми слоями добавлено пропадание 50%.
Вы также можете думать об отсеве как о создании своего рода ансамбля сетей. Прогнозы больше не будут делать одна 
большая сеть, а будет комитет меньших сетей. Члены комитета склонны совершать разные ошибки, но в то же время правы, 
что делает комитет в целом лучше, чем любой человек. (Если вам знакомы случайные леса как совокупность деревьев 
решений, идея та же.)   

### Добавление отсева
В Keras частота аргументов коэффициента отсева определяет, какой процент блоков ввода необходимо отключить. 
Поместите слой исключения непосредственно перед слоем, к которому вы хотите применить исключение: 

```python
keras.Sequential([
    # ...
    layers.Dropout(rate=0.3), # применить 30% dropout к следующему слою
    layers.Dense(16),
    # ...
])
```
### Пакетная нормализация
Следующий специальный слой, который мы рассмотрим, выполняет «пакетную нормализацию» (или «батчнорм»), которая может 
помочь исправить медленное или нестабильное обучение. 

С нейронными сетями, как правило, неплохо поместить все ваши данные в общий масштаб, возможно, с помощью чего-то 
вроде StandardScaler или MinMaxScaler от scikit-learn. Причина в том, что SGD смещает веса сети пропорционально тому,
насколько большую активацию производят данные. Функции, которые имеют тенденцию вызывать активацию самого разного 
размера, могут привести к нестабильному тренировочному поведению.   

Теперь, если хорошо нормализовать данные до того, как они попадут в сеть, возможно, также будет лучше нормализовать 
внутри сети! Фактически, у нас есть особый вид слоя, который может это делать, - слой пакетной нормализации. Слой 
нормализации пакета просматривает каждый пакет по мере его поступления, сначала нормализуя пакет с его собственным 
средним значением и стандартным отклонением, а затем также помещает данные в новый масштаб с двумя обучаемыми 
параметрами изменения масштаба. Batchnorm, по сути, выполняет своего рода скоординированное изменение масштаба своих 
входных данных.     

Чаще всего batchnorm добавляется для помощи в процессе оптимизации (хотя иногда он также может помочь в 
прогнозировании). Моделям с батчнормом, как правило, требуется меньше эпох для завершения обучения. Более того, 
батчнорм может также исправить различные проблемы, которые могут привести к "застреванию" тренировки. Подумайте о 
добавлении пакетной нормализации к своим моделям, особенно если у вас возникли проблемы во время обучения.   

### Добавление пакетной нормализации
Кажется, что пакетную нормализацию можно использовать практически в любой точке сети. Можно положить после слоя ...

```python
layers.Dense(16, activation='relu'),
layers.BatchNormalization(),
```
... или между слоем и его функцией активации:

```python
layers.Dense(16),
layers.BatchNormalization(),
layers.Activation('relu'),
```
И если вы добавите его в качестве первого уровня своей сети, он может действовать как своего рода адаптивный 
препроцессор, заменяющий что-то вроде StandardScaler от Sci-Kit Learn. 

### Пример - использование исключения и пакетной нормализации
Продолжим разработку модели Red Wine. Теперь мы еще больше увеличим емкость, но добавим отсев для контроля 
переобучения и пакетную нормализацию, чтобы ускорить оптимизацию. На этот раз мы также откажемся от стандартизации 
данных, чтобы продемонстрировать, как пакетная нормализация может стабилизировать обучение.  
```python
# Setup plotting
import matplotlib.pyplot as plt

plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)


import pandas as pd
red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')

# Create training and validation splits
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)

# Split features and target
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```
При добавлении исключения вам может потребоваться увеличить количество единиц в ваших плотных слоях.
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])
```
На этот раз нечего изменить в том, как мы организовали обучение.
```python
model.compile(
    optimizer='adam',
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=100,
    verbose=0,
)


# Show the learning curves
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();

```
Как правило, вы получите лучшую производительность, если стандартизируете свои данные перед их использованием для 
обучения. Однако то, что мы вообще смогли использовать необработанные данные, показывает, насколько эффективной 
может быть пакетная нормализация для более сложных наборов данных.  
