# Word Vectors
Вложения слов
На данный момент вы знаете, что машинное обучение текста требует, чтобы вы сначала представили текст в числовом виде.
До сих пор вы делали это с помощью набора словесных представлений. Но обычно лучше получается встраивание слов. 

Вложения слов (также называемые векторами слов) представляют каждое слово в числовом виде таким образом, что вектор 
соответствует тому, как это слово используется или что оно означает. Векторные кодировки изучаются с учетом 
контекста, в котором появляются слова. Слова, встречающиеся в похожих контекстах, будут иметь похожие векторы. 
Например, векторы для «леопард», «лев» и «тигр» будут близко друг к другу, в то время как они будут далеко от 
«планеты» и «замка».    

Более того, отношения между словами можно исследовать с помощью математических операций. Вычитание векторов для 
«мужчины» и «женщины» вернет другой вектор. Если вы добавите это к вектору «короля», результат будет близок к 
вектору «ферзя».  

Эти векторы можно использовать как функции для моделей машинного обучения. Векторы слов обычно улучшают 
производительность ваших моделей выше кодировки слов. spaCy предоставляет вложения, полученные из модели Word2Vec. 
Вы можете получить к ним доступ, загрузив большую языковую модель, например en_core_web_lg. Тогда они будут доступны 
для токенов из атрибута .vector.   
```python
import numpy as np
import spacy

# Need to load the large model to get the vectors
nlp = spacy.load('en_core_web_lg')
```
```python
# Disabling other pipes because we don't need them and it'll speed up this part a bit
text = "These vectors can be used as features for machine learning models."
with nlp.disable_pipes():
    vectors = np.array([token.vector for token in  nlp(text)])
```
```python
vectors.shape
```
Это 300-мерные векторы, по одному на каждое слово. Однако у нас есть только метки на уровне документа, и наши модели 
не смогут использовать вложения на уровне слов. Итак, вам нужно векторное представление для всего документа.  

Есть много способов объединить все векторы слов в единый вектор документа, который мы можем использовать для 
обучения модели. Простой и удивительно эффективный подход - просто усреднить векторы для каждого слова в документе. 
Затем вы можете использовать эти векторы документов для моделирования.  

spaCy вычисляет средний вектор документа, который вы можете получить с помощью doc.vector. Вот пример загрузки 
данных о спаме и их преобразования в векторы документов.  

```python
import pandas as pd

# Loading the spam data
# ham is the label for non-spam messages
spam = pd.read_csv('../input/nlp-course/spam.csv')

with nlp.disable_pipes():
    doc_vectors = np.array([nlp(text).vector for text in spam.text])
    
doc_vectors.shape
```
### Классификационные модели
С помощью векторов документов вы можете обучать модели scikit-learn, модели xgboost или любой другой стандартный 
подход к моделированию 

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,
                                                    test_size=0.1, random_state=1)
```
Scikit-learn предоставляет SVM-классификатор LinearSVC. Это работает аналогично другим моделям scikit-learn.

```python
from sklearn.svm import LinearSVC

# Set dual=False to speed up training, and it's not needed
svc = LinearSVC(random_state=1, dual=False, max_iter=10000)
svc.fit(X_train, y_train)
print(f"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%", )
```
### Сходство документов
Документы с похожим содержанием обычно имеют похожие векторы. Таким образом, вы можете найти похожие документы, 
измерив сходство между векторами. Общей метрикой для этого является косинусное подобие, которое измеряет угол между 
двумя векторами, a и b.  

cosθ = a⋅b∥a∥∥b∥
 
Это скалярное произведение a и b, деленное на величины каждого вектора. Косинусное подобие может варьироваться от -1 
до 1, что соответствует полной противоположности идеального сходства, соответственно. Для его расчета вы можете 
использовать метрику из scikit-learn или написать свою собственную функцию.  
```python
def cosine_similarity(a, b):
    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))
a = nlp("REPLY NOW FOR FREE TEA").vector
b = nlp("According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.").vector
cosine_similarity(a, b)
```


